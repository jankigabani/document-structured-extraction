{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965c0694",
   "metadata": {},
   "source": [
    "## working for resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da1594ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: PyMuPDF in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: docx2txt in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (0.9)\n",
      "Requirement already satisfied: pandas in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: markdown in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: pydantic in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (2.11.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-text-splitters) (0.3.69)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.11.0)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.4.1)\n",
      "Requirement already satisfied: anyio in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (4.9.0)\n",
      "Requirement already satisfied: certifi in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.3.1)\n",
      "All packages installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \"langchain[groq]\"\n",
    "!pip install tiktoken PyMuPDF docx2txt pandas markdown pydantic\n",
    "!pip install langchain-text-splitters\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a23f2c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import mimetypes\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import markdown\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0c7dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API key configured!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GROQ_API_KEY = \"groq_api_key\"\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "print(\"Groq API key configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a7ca104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def identify_file_type(file_path):\n",
    "    \"\"\"Identify file type based on extension.\"\"\"\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return \"pdf\"\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        return \"docx\"\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        return \"csv\"\n",
    "    elif file_path.endswith(\".md\"):\n",
    "        return \"md\"\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        return \"txt\"\n",
    "    elif file_path.endswith(\".json\"):\n",
    "        return \"json\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "def parse_file(file_path, file_type):\n",
    "    \"\"\"Parse various file types and extract text content.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    if file_type == \"pdf\":\n",
    "        text = \"\"\n",
    "        with fitz.open(file_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text.strip()\n",
    "\n",
    "    elif file_type == \"docx\":\n",
    "        return docx2txt.process(file_path).strip()\n",
    "\n",
    "    elif file_type == \"csv\":\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df.to_string(index=False)\n",
    "\n",
    "    elif file_type == \"md\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return markdown.markdown(f.read())\n",
    "\n",
    "    elif file_type == \"txt\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "\n",
    "    elif file_type == \"json\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            return json.dumps(data, indent=2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "print(\"File processing functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12eecb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counting and chunking functions defined!\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text, model_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model_name)\n",
    "        return len(enc.encode(text))\n",
    "    except:\n",
    "        # Fallback estimation\n",
    "        return int(len(text.split()) * 1.3)\n",
    "\n",
    "def chunk_text(text, chunk_size=15000, chunk_overlap=500):\n",
    "    \"\"\"Split text into manageable chunks.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=count_tokens\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "print(\"Token counting and chunking functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "272fa238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume schema loaded successfully!\n",
      "Schema has 5 main sections\n"
     ]
    }
   ],
   "source": [
    "RESUME_SCHEMA = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n",
    "    \"additionalProperties\": False,\n",
    "    \"definitions\": {\n",
    "        \"iso8601\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"e.g. 2014-06-29\",\n",
    "            \"pattern\": \"^([1-2][0-9]{3}-[0-1][0-9]-[0-3][0-9]|[1-2][0-9]{3}-[0-1][0-9]|[1-2][0-9]{3})$\"\n",
    "        }\n",
    "    },\n",
    "    \"properties\": {\n",
    "        \"basics\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": True,\n",
    "            \"properties\": {\n",
    "                \"name\": {\"type\": \"string\"},\n",
    "                \"label\": {\"type\": \"string\", \"description\": \"e.g. Web Developer\"},\n",
    "                \"image\": {\"type\": \"string\", \"description\": \"URL to image\"},\n",
    "                \"email\": {\"type\": \"string\", \"format\": \"email\"},\n",
    "                \"phone\": {\"type\": \"string\"},\n",
    "                \"url\": {\"type\": \"string\", \"format\": \"uri\"},\n",
    "                \"summary\": {\"type\": \"string\", \"description\": \"Write a short 2-3 sentence biography\"},\n",
    "                \"location\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"address\": {\"type\": \"string\"},\n",
    "                        \"postalCode\": {\"type\": \"string\"},\n",
    "                        \"city\": {\"type\": \"string\"},\n",
    "                        \"countryCode\": {\"type\": \"string\"},\n",
    "                        \"region\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                },\n",
    "                \"profiles\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"network\": {\"type\": \"string\"},\n",
    "                            \"username\": {\"type\": \"string\"},\n",
    "                            \"url\": {\"type\": \"string\", \"format\": \"uri\"}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"work\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"description\": {\"type\": \"string\"},\n",
    "                    \"position\": {\"type\": \"string\"},\n",
    "                    \"url\": {\"type\": \"string\", \"format\": \"uri\"},\n",
    "                    \"startDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"endDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"summary\": {\"type\": \"string\"},\n",
    "                    \"highlights\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"education\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"institution\": {\"type\": \"string\"},\n",
    "                    \"url\": {\"type\": \"string\", \"format\": \"uri\"},\n",
    "                    \"area\": {\"type\": \"string\"},\n",
    "                    \"studyType\": {\"type\": \"string\"},\n",
    "                    \"startDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"endDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"score\": {\"type\": \"string\"},\n",
    "                    \"courses\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"skills\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"level\": {\"type\": \"string\"},\n",
    "                    \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"projects\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"description\": {\"type\": \"string\"},\n",
    "                    \"highlights\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"startDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"endDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"url\": {\"type\": \"string\", \"format\": \"uri\"},\n",
    "                    \"roles\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"entity\": {\"type\": \"string\"},\n",
    "                    \"type\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"title\": \"Resume Schema\",\n",
    "    \"type\": \"object\"\n",
    "}\n",
    "\n",
    "print(\"Resume schema loaded successfully!\")\n",
    "print(f\"Schema has {len(RESUME_SCHEMA['properties'])} main sections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45f6b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Pydantic models defined for structured extraction!\n",
      "LLM re-initialized with fixed tool calling capabilities!\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class BasicInfo(BaseModel):\n",
    "    \"\"\"Extract basic personal information from resume.\"\"\"\n",
    "    name: Optional[str] = Field(default=\"\", description=\"Full name of the person\")\n",
    "    email: Optional[str] = Field(default=\"\", description=\"Email address\")\n",
    "    phone: Optional[str] = Field(default=\"\", description=\"Phone number\")\n",
    "    location: Optional[str] = Field(default=\"\", description=\"Location/Address\")\n",
    "    summary: Optional[str] = Field(default=\"\", description=\"Professional summary or objective\")\n",
    "\n",
    "class WorkExperience(BaseModel):\n",
    "    \"\"\"Extract work experience information.\"\"\"\n",
    "    company_name: Optional[str] = Field(default=\"\", description=\"Name of the company\")\n",
    "    position: Optional[str] = Field(default=\"\", description=\"Job title/position\")\n",
    "    start_date: Optional[str] = Field(default=\"\", description=\"Start date in YYYY-MM format\")\n",
    "    end_date: Optional[str] = Field(default=\"\", description=\"End date in YYYY-MM format or 'Present'\")\n",
    "    description: Optional[str] = Field(default=\"\", description=\"Job description and responsibilities\")\n",
    "    highlights: List[str] = Field(default=[], description=\"Key achievements or highlights\")\n",
    "\n",
    "class Education(BaseModel):\n",
    "    \"\"\"Extract education information.\"\"\"\n",
    "    institution: Optional[str] = Field(default=\"\", description=\"Name of educational institution\")\n",
    "    degree: Optional[str] = Field(default=\"\", description=\"Degree or study type\")\n",
    "    field_of_study: Optional[str] = Field(default=\"\", description=\"Field of study or area\")\n",
    "    start_date: Optional[str] = Field(default=\"\", description=\"Start date in YYYY format\")\n",
    "    end_date: Optional[str] = Field(default=\"\", description=\"End date in YYYY format\")\n",
    "    score: Optional[str] = Field(default=\"\", description=\"GPA or score if mentioned\")\n",
    "\n",
    "class Skills(BaseModel):\n",
    "    \"\"\"Extract skills information.\"\"\"\n",
    "    category: Optional[str] = Field(default=\"\", description=\"Skill category (e.g., Technical, Programming)\")\n",
    "    skills_list: List[str] = Field(default=[], description=\"List of skills in this category\")\n",
    "\n",
    "class Projects(BaseModel):\n",
    "    \"\"\"Extract project information.\"\"\"\n",
    "    name: Optional[str] = Field(default=\"\", description=\"Project name\")\n",
    "    description: Optional[str] = Field(default=\"\", description=\"Project description\")\n",
    "    technologies: List[str] = Field(default=[], description=\"Technologies used\")\n",
    "    highlights: List[str] = Field(default=[], description=\"Key achievements or features\")\n",
    "    start_date: Optional[str] = Field(default=\"\", description=\"Start date\")\n",
    "    end_date: Optional[str] = Field(default=\"\", description=\"End date\")\n",
    "\n",
    "tools = [BasicInfo, WorkExperience, Education, Skills, Projects]\n",
    "\n",
    "print(\"Fixed Pydantic models defined for structured extraction!\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "print(\"LLM re-initialized with fixed tool calling capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70f337ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized with tool calling capabilities!\n"
     ]
    }
   ],
   "source": [
    "llm = init_chat_model(\"meta-llama/llama-4-maverick-17b-128e-instruct\", model_provider=\"groq\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"LLM initialized with tool calling capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99585641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Complexity Analysis:\n",
      "- Nested objects: 8\n",
      "- Max nesting level: 7\n",
      "- Schema token count: 850\n",
      "- Complexity: complex\n",
      "- Recommended chunks: 3\n"
     ]
    }
   ],
   "source": [
    "def analyze_schema_complexity(schema: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze JSON schema complexity to determine processing strategy.\"\"\"\n",
    "    \n",
    "    def count_nested_objects(obj, level=0):\n",
    "        count = 0\n",
    "        max_depth = level\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            if obj.get('type') == 'object':\n",
    "                count += 1\n",
    "            \n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    nested_count, nested_depth = count_nested_objects(value, level + 1)\n",
    "                    count += nested_count\n",
    "                    max_depth = max(max_depth, nested_depth)\n",
    "        \n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    nested_count, nested_depth = count_nested_objects(item, level + 1)\n",
    "                    count += nested_count\n",
    "                    max_depth = max(max_depth, nested_depth)\n",
    "        \n",
    "        return count, max_depth\n",
    "    \n",
    "    nested_objects, max_nesting_level = count_nested_objects(schema)\n",
    "    schema_str = json.dumps(schema)\n",
    "    schema_token_count = count_tokens(schema_str)\n",
    "    \n",
    "    if nested_objects < 20 and max_nesting_level < 4:\n",
    "        complexity = \"simple\"\n",
    "        recommended_chunks = 1\n",
    "    elif nested_objects < 50 and max_nesting_level < 6:\n",
    "        complexity = \"medium\"\n",
    "        recommended_chunks = 2\n",
    "    else:\n",
    "        complexity = \"complex\"\n",
    "        recommended_chunks = 3\n",
    "    \n",
    "    return {\n",
    "        \"nested_objects\": nested_objects,\n",
    "        \"max_nesting_level\": max_nesting_level,\n",
    "        \"schema_token_count\": schema_token_count,\n",
    "        \"complexity\": complexity,\n",
    "        \"recommended_chunks\": recommended_chunks\n",
    "    }\n",
    "\n",
    "complexity_analysis = analyze_schema_complexity(RESUME_SCHEMA)\n",
    "print(\"Schema Complexity Analysis:\")\n",
    "print(f\"- Nested objects: {complexity_analysis['nested_objects']}\")\n",
    "print(f\"- Max nesting level: {complexity_analysis['max_nesting_level']}\")\n",
    "print(f\"- Schema token count: {complexity_analysis['schema_token_count']}\")\n",
    "print(f\"- Complexity: {complexity_analysis['complexity']}\")\n",
    "print(f\"- Recommended chunks: {complexity_analysis['recommended_chunks']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09c2ebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved conversion function defined!\n"
     ]
    }
   ],
   "source": [
    "def convert_text_to_json(text: str, schema: Dict[str, Any], confidence_threshold: float = 0.7):\n",
    "    \"\"\"\n",
    "    Convert unstructured text to structured JSON following the given schema.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to convert\n",
    "        schema: JSON schema to follow\n",
    "        confidence_threshold: Minimum confidence for field acceptance\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing the structured data and confidence scores\n",
    "    \"\"\"\n",
    "    \n",
    "    complexity_analysis = analyze_schema_complexity(schema)\n",
    "    print(f\"Processing with {complexity_analysis['complexity']} complexity...\")\n",
    "    \n",
    "    text_tokens = count_tokens(text)\n",
    "    print(f\"Input text tokens: {text_tokens}\")\n",
    "    \n",
    "    max_context = 25000  \n",
    "    \n",
    "    if text_tokens > max_context:\n",
    "        print(f\"Text too large ({text_tokens} tokens), chunking into smaller parts...\")\n",
    "        chunks = chunk_text(text, chunk_size=15000, chunk_overlap=500)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "    else:\n",
    "        chunks = [text]\n",
    "        print(\"Processing as single chunk\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\nProcessing chunk {i+1}/{len(chunks)}...\")\n",
    "        \n",
    "        extraction_prompt = f\"\"\"\n",
    "        You are an expert at extracting structured information from unstructured text.\n",
    "        \n",
    "        Extract information from the following text and structure it according to the provided tools.\n",
    "        \n",
    "        IMPORTANT INSTRUCTIONS:\n",
    "        1. Extract ALL relevant information you can find\n",
    "        2. Use empty string (\"\") for missing text fields, not null\n",
    "        3. Use empty array ([]) for missing list fields, not null\n",
    "        4. Format dates as YYYY-MM-DD, YYYY-MM, or YYYY when available\n",
    "        5. If information is not available in the text, provide empty string/array as appropriate\n",
    "        6. Be thorough and accurate - extract everything you can see\n",
    "        7. For work experience, extract company name, position, dates, and all bullet points\n",
    "        8. For education, extract institution, degree, field of study, and scores\n",
    "        9. For projects, extract name, description, technologies, and achievements\n",
    "        10. For skills, group them into logical categories\n",
    "        \n",
    "        Text to extract from:\n",
    "        {chunk}\n",
    "        \n",
    "        Use the provided tools to extract information systematically. Call each tool type multiple times if you find multiple instances (e.g., multiple work experiences, education entries, etc.).\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [HumanMessage(extraction_prompt)]\n",
    "        \n",
    "        try:\n",
    "            ai_response = llm_with_tools.invoke(messages)\n",
    "            \n",
    "            if hasattr(ai_response, 'tool_calls') and ai_response.tool_calls:\n",
    "                print(f\"Found {len(ai_response.tool_calls)} tool calls\")\n",
    "                chunk_results = {}\n",
    "                \n",
    "                for tool_call in ai_response.tool_calls:\n",
    "                    tool_name = tool_call['name']\n",
    "                    tool_args = tool_call['args']\n",
    "                    print(f\"Extracted {tool_name}: {len(str(tool_args))} characters\")\n",
    "                    \n",
    "                    if tool_name not in chunk_results:\n",
    "                        chunk_results[tool_name] = []\n",
    "                    chunk_results[tool_name].append(tool_args)\n",
    "                \n",
    "                all_results.append(chunk_results)\n",
    "            else:\n",
    "                print(\"No structured data extracted from this chunk\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if all_results:\n",
    "        final_result = merge_chunk_results(all_results)\n",
    "        return final_result\n",
    "    else:\n",
    "        print(\"No results extracted from any chunks\")\n",
    "        return {\n",
    "            \"basics\": {},\n",
    "            \"work\": [],\n",
    "            \"education\": [],\n",
    "            \"skills\": [],\n",
    "            \"projects\": []\n",
    "        }\n",
    "\n",
    "def merge_chunk_results(chunk_results: List[Dict]) -> Dict:\n",
    "    \"\"\"Merge results from multiple chunks into final structured format.\"\"\"\n",
    "    \n",
    "    merged = {\n",
    "        \"basics\": {},\n",
    "        \"work\": [],\n",
    "        \"education\": [],\n",
    "        \"skills\": [],\n",
    "        \"projects\": []\n",
    "    }\n",
    "    \n",
    "    for chunk_result in chunk_results:\n",
    "        if \"BasicInfo\" in chunk_result:\n",
    "            for basic_info in chunk_result[\"BasicInfo\"]:\n",
    "                for key, value in basic_info.items():\n",
    "                    if value and str(value).strip():  \n",
    "                        merged[\"basics\"][key] = value\n",
    "        \n",
    "        if \"WorkExperience\" in chunk_result:\n",
    "            for work in chunk_result[\"WorkExperience\"]:\n",
    "                merged[\"work\"].append({\n",
    "                    \"name\": work.get(\"company_name\", \"\"),\n",
    "                    \"position\": work.get(\"position\", \"\"),\n",
    "                    \"startDate\": work.get(\"start_date\", \"\"),\n",
    "                    \"endDate\": work.get(\"end_date\", \"\"),\n",
    "                    \"summary\": work.get(\"description\", \"\"),\n",
    "                    \"highlights\": work.get(\"highlights\", [])\n",
    "                })\n",
    "        \n",
    "        if \"Education\" in chunk_result:\n",
    "            for edu in chunk_result[\"Education\"]:\n",
    "                merged[\"education\"].append({\n",
    "                    \"institution\": edu.get(\"institution\", \"\"),\n",
    "                    \"studyType\": edu.get(\"degree\", \"\"),\n",
    "                    \"area\": edu.get(\"field_of_study\", \"\"),\n",
    "                    \"startDate\": edu.get(\"start_date\", \"\"),\n",
    "                    \"endDate\": edu.get(\"end_date\", \"\"),\n",
    "                    \"score\": edu.get(\"score\", \"\")\n",
    "                })\n",
    "        \n",
    "        if \"Skills\" in chunk_result:\n",
    "            for skill in chunk_result[\"Skills\"]:\n",
    "                merged[\"skills\"].append({\n",
    "                    \"name\": skill.get(\"category\", \"\"),\n",
    "                    \"keywords\": skill.get(\"skills_list\", [])\n",
    "                })\n",
    "        \n",
    "        if \"Projects\" in chunk_result:\n",
    "            for project in chunk_result[\"Projects\"]:\n",
    "                merged[\"projects\"].append({\n",
    "                    \"name\": project.get(\"name\", \"\"),\n",
    "                    \"description\": project.get(\"description\", \"\"),\n",
    "                    \"keywords\": project.get(\"technologies\", []),\n",
    "                    \"highlights\": project.get(\"highlights\", []),\n",
    "                    \"startDate\": project.get(\"start_date\", \"\"),\n",
    "                    \"endDate\": project.get(\"end_date\", \"\")\n",
    "                })\n",
    "    \n",
    "    return merged\n",
    "\n",
    "print(\"Improved conversion function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2a38636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete pipeline to process a resume file and convert to structured JSON.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the resume file\n",
    "        \n",
    "    Returns:\n",
    "        Structured JSON data following the resume schema\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Processing resume: {file_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        file_type = identify_file_type(file_path)\n",
    "        text = parse_file(file_path, file_type)\n",
    "        \n",
    "        print(f\"File parsed successfully!\")\n",
    "        print(f\"   - File type: {file_type}\")\n",
    "        print(f\"   - Text length: {len(text)} characters\")\n",
    "        print(f\"   - Token count: {count_tokens(text)}\")\n",
    "        \n",
    "        structured_data = convert_text_to_json(text, RESUME_SCHEMA)\n",
    "        \n",
    "        print(\"\\nConversion completed!\")\n",
    "        \n",
    "        print(\"\\n🔍 Step 3: Basic validation...\")\n",
    "        required_sections = [\"basics\", \"work\", \"education\", \"skills\", \"projects\"]\n",
    "        \n",
    "        validation_results = {}\n",
    "        for section in required_sections:\n",
    "            if section in structured_data and structured_data[section]:\n",
    "                validation_results[section] = \"✅ Present\"\n",
    "            else:\n",
    "                validation_results[section] = \"⚠️ Missing or empty\"\n",
    "        \n",
    "        print(\"Validation Results:\")\n",
    "        for section, status in validation_results.items():\n",
    "            print(f\"   - {section}: {status}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"data\": structured_data,\n",
    "            \"validation\": validation_results,\n",
    "            \"metadata\": {\n",
    "                \"file_type\": file_type,\n",
    "                \"original_length\": len(text),\n",
    "                \"token_count\": count_tokens(text),\n",
    "                \"processing_timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file: {str(e)}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"data\": None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df8c2e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing resume: C:\\Users\\Admin\\OneDrive\\Desktop\\JankiGabaniResume_.pdf\n",
      "==================================================\n",
      "File parsed successfully!\n",
      "   - File type: pdf\n",
      "   - Text length: 4587 characters\n",
      "   - Token count: 996\n",
      "Processing with complex complexity...\n",
      "Input text tokens: 996\n",
      "Processing as single chunk\n",
      "\n",
      "Processing chunk 1/1...\n",
      "Found 11 tool calls\n",
      "Extracted BasicInfo: 119 characters\n",
      "Extracted Skills: 473 characters\n",
      "Extracted Skills: 186 characters\n",
      "Extracted WorkExperience: 1049 characters\n",
      "Extracted WorkExperience: 729 characters\n",
      "Extracted WorkExperience: 497 characters\n",
      "Extracted Education: 169 characters\n",
      "Extracted Education: 138 characters\n",
      "Extracted Education: 143 characters\n",
      "Extracted Projects: 348 characters\n",
      "Extracted Projects: 474 characters\n",
      "\n",
      "Conversion completed!\n",
      "\n",
      "🔍 Step 3: Basic validation...\n",
      "Validation Results:\n",
      "   - basics: ✅ Present\n",
      "   - work: ✅ Present\n",
      "   - education: ✅ Present\n",
      "   - skills: ✅ Present\n",
      "   - projects: ✅ Present\n",
      "\n",
      "SUCCESS! Resume processed successfully!\n",
      "\n",
      " EXTRACTED DATA:\n",
      "==================================================\n",
      "{\n",
      "  \"basics\": {\n",
      "    \"email\": \"jankigabani10@gmail.com\",\n",
      "    \"name\": \"Janki Gabani\",\n",
      "    \"phone\": \"(+91)9638183121\"\n",
      "  },\n",
      "  \"work\": [\n",
      "    {\n",
      "      \"name\": \"Casepoint Pvt. Ltd.\",\n",
      "      \"position\": \"Associate Data Scientist\",\n",
      "      \"startDate\": \"2024-01\",\n",
      "      \"endDate\": \"\",\n",
      "      \"summary\": \"\",\n",
      "      \"highlights\": [\n",
      "        \"Built and optimized FastAPI-based RAG pipelines for intelligent Q&A across billions of documents, achieving a ∼80% answer accuracy using fine-tuned local LLaMA3.1 models.\",\n",
      "        \"Led development of document summarization features in the FOIA software for U.S. Government, enhancing document triage efficiency by over65%.\",\n",
      "        \"Developed scalable API Gateway for parallel processing and bulk requests, improving throughput by3x using Docker-based microservices architecture.\",\n",
      "        \"Implemented LangChain with Tiktoken for token usage tracking and optimization, reducing token usage cost by28%.\",\n",
      "        \"Integrated ElasticSearch-powered RAG retrieval for semantic document matching with millisecond latency and high recall.\",\n",
      "        \"Designed a custom Text-to-SQL engine enabling dynamic querying across multiple databases; enhanced performance and query accuracy using internal logic rules and validation guardrails.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"LaMinds\",\n",
      "      \"position\": \"Intern\",\n",
      "      \"startDate\": \"2023-04\",\n",
      "      \"endDate\": \"2023-06\",\n",
      "      \"summary\": \"\",\n",
      "      \"highlights\": [\n",
      "        \"Fine-tuned FLUX image generation model on a custom dataset using LoRA adapters to generate domain-specific outputs.\",\n",
      "        \"Developed a mobile UI component detector using YOLOv8 for GUI Agent prototyping, achieving ∼85% detection accuracy.\",\n",
      "        \"Implemented OCR pipelines using Hugging Face models to extract text from complex image layouts with improved recognition rates.\",\n",
      "        \"Performed comprehensive API testing via Postman to validate model endpoints and integration workflows.\",\n",
      "        \"Worked on SQL-based backend operations; transformed and visualized data insights using Tableau for internal reporting.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Chegg India Pvt. Ltd.\",\n",
      "      \"position\": \"Subject Matter Expert (Freelancing)\",\n",
      "      \"startDate\": \"2022-03\",\n",
      "      \"endDate\": \"2023-02\",\n",
      "      \"summary\": \"\",\n",
      "      \"highlights\": [\n",
      "        \"Provided solutions to students’ queries as a Computer Science Subject Matter Expert and offered in-depth clarification and guidance.\",\n",
      "        \"Leveraged problem-solving abilities to address diverse academic challenges effectively. Played a key role in enhancing student’s understanding through clear and concise explanations.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"institution\": \"P P Savani University\",\n",
      "      \"studyType\": \"Bachelor of Technology\",\n",
      "      \"area\": \"Computer Engineering\",\n",
      "      \"startDate\": \"\",\n",
      "      \"endDate\": \"\",\n",
      "      \"score\": \"8.75\"\n",
      "    },\n",
      "    {\n",
      "      \"institution\": \"Matrubhumi Vidhyalaya\",\n",
      "      \"studyType\": \"Class XII\",\n",
      "      \"area\": \"\",\n",
      "      \"startDate\": \"\",\n",
      "      \"endDate\": \"\",\n",
      "      \"score\": \"70.87%\"\n",
      "    },\n",
      "    {\n",
      "      \"institution\": \"V N Godhani Kanya Vidhyalaya\",\n",
      "      \"studyType\": \"Class X\",\n",
      "      \"area\": \"\",\n",
      "      \"startDate\": \"\",\n",
      "      \"endDate\": \"\",\n",
      "      \"score\": \"90.33%\"\n",
      "    }\n",
      "  ],\n",
      "  \"skills\": [\n",
      "    {\n",
      "      \"name\": \"Technical\",\n",
      "      \"keywords\": [\n",
      "        \"Python\",\n",
      "        \"FastAPI\",\n",
      "        \"Flask\",\n",
      "        \"Next.js\",\n",
      "        \"MongoDB\",\n",
      "        \"PostgreSQL\",\n",
      "        \"Elastic Search\",\n",
      "        \"Docker\",\n",
      "        \"Object-Oriented Programming\",\n",
      "        \"AWS SageMaker\",\n",
      "        \"Prompt Engineering\",\n",
      "        \"LLMs\",\n",
      "        \"RAG pipelines\",\n",
      "        \"fine-tuning\",\n",
      "        \"CrewAI\",\n",
      "        \"Multi-Agent Pipelines\",\n",
      "        \"Computer Vision\",\n",
      "        \"Machine Learning\",\n",
      "        \"Deep Learning Architectures\",\n",
      "        \"Exploratory Data Analysis\",\n",
      "        \"Data Storytelling\",\n",
      "        \"Data Visualization\",\n",
      "        \"Tableau\",\n",
      "        \"Model Deployment\",\n",
      "        \"Version Control\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Non Technical\",\n",
      "      \"keywords\": [\n",
      "        \"Leadership\",\n",
      "        \"communication\",\n",
      "        \"teamwork\",\n",
      "        \"community engagement\",\n",
      "        \"policy management\",\n",
      "        \"project management\",\n",
      "        \"networking\",\n",
      "        \"partnerships\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"name\": \"Auto Encoder Self-Driving Car\",\n",
      "      \"description\": \"Created and trained an autoencoder model for a self-driving car project\",\n",
      "      \"keywords\": [\n",
      "        \"Jupyter Notebook\",\n",
      "        \"Amazon SageMaker\"\n",
      "      ],\n",
      "      \"highlights\": [\n",
      "        \"Applied deep learning techniques to enhance the car’s perception and decision-making capabilities.\"\n",
      "      ],\n",
      "      \"startDate\": \"\",\n",
      "      \"endDate\": \"2023-09\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SwasthKAI\",\n",
      "      \"description\": \"Developed SwasthaKAI, a cutting-edge nutrition app employing Python, OpenCV, and a web scraping approach.\",\n",
      "      \"keywords\": [\n",
      "        \"Python\",\n",
      "        \"OpenCV\",\n",
      "        \"web scraping\"\n",
      "      ],\n",
      "      \"highlights\": [\n",
      "        \"Enabled users to scan food packaging, access detailed ingredient data, and assess health impacts\",\n",
      "        \"Achieved a remarkable25% improvement in system Accuracy and gained87% precision, ensuring scalable maintainability\"\n",
      "      ],\n",
      "      \"startDate\": \"\",\n",
      "      \"endDate\": \"2023-03\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "   - File type: pdf\n",
      "   - Original length: 4587 chars\n",
      "   - Token count: 996\n",
      "   - Processing time: 2025-07-21T07:19:43.824136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resume_file_path = \"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\JankiGabaniResume_.pdf\"\n",
    "# resume_file_path = \"data\\\\deedy-resume-reversed.pdf\"\n",
    "\n",
    "result = process_resume_file(resume_file_path)\n",
    "\n",
    "if result[\"status\"] == \"success\":\n",
    "    print(\"\\nSUCCESS! Resume processed successfully!\")\n",
    "    print(\"\\n EXTRACTED DATA:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    structured_json = json.dumps(result[\"data\"], indent=2, ensure_ascii=False)\n",
    "    print(structured_json)\n",
    "    \n",
    "    print(f\"\\nMETADATA:\")\n",
    "    print(f\"   - File type: {result['metadata']['file_type']}\")\n",
    "    print(f\"   - Original length: {result['metadata']['original_length']} chars\")\n",
    "    print(f\"   - Token count: {result['metadata']['token_count']}\")\n",
    "    print(f\"   - Processing time: {result['metadata']['processing_timestamp']}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nFAILED to process resume:\")\n",
    "    print(f\"   Error: {result['error']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "176e1512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: extracted_resume.json\n",
      "Full results with metadata saved to: extraction_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if result[\"status\"] == \"success\":\n",
    "    output_filename = \"extracted_resume.json\"\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result[\"data\"], f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Results saved to: {output_filename}\")\n",
    "    \n",
    "    metadata_filename = \"extraction_metadata.json\"\n",
    "    with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Full results with metadata saved to: {metadata_filename}\")\n",
    "else:\n",
    "    print(\"No results to save due to processing error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
