{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965c0694",
   "metadata": {},
   "source": [
    "## working for resume & md file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "da1594ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: PyMuPDF in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: docx2txt in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (0.9)\n",
      "Requirement already satisfied: pandas in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: markdown in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: pydantic in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (2.11.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-text-splitters) (0.3.69)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.11.0)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.4.1)\n",
      "Requirement already satisfied: anyio in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (4.9.0)\n",
      "Requirement already satisfied: certifi in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\janki\\ai projects\\ai-solution-design\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.3.1)\n",
      "All packages installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \"langchain[groq]\"\n",
    "!pip install tiktoken PyMuPDF docx2txt pandas markdown pydantic\n",
    "!pip install langchain-text-splitters\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a23f2c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import mimetypes\n",
    "from typing import Dict, List, Any, Optional, Union\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import markdown\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c7dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq API key configured!\n"
     ]
    }
   ],
   "source": [
    "GROQ_API_KEY = \"groq_api_key\"\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "\n",
    "print(\"Groq API key configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API key configured!\n"
     ]
    }
   ],
   "source": [
    "GEMINI_API_KEY = \"gemini_api_key\"\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "\n",
    "print(\"Gemini API key configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1a7ca104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def identify_file_type(file_path):\n",
    "    \"\"\"Identify file type based on extension.\"\"\"\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        return \"pdf\"\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        return \"docx\"\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        return \"csv\"\n",
    "    elif file_path.endswith(\".md\"):\n",
    "        return \"md\"\n",
    "    elif file_path.endswith(\".txt\"):\n",
    "        return \"txt\"\n",
    "    elif file_path.endswith(\".json\"):\n",
    "        return \"json\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "def parse_file(file_path, file_type):\n",
    "    \"\"\"Parse various file types and extract text content.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    if file_type == \"pdf\":\n",
    "        text = \"\"\n",
    "        with fitz.open(file_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text.strip()\n",
    "\n",
    "    elif file_type == \"docx\":\n",
    "        return docx2txt.process(file_path).strip()\n",
    "\n",
    "    elif file_type == \"csv\":\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df.to_string(index=False)\n",
    "\n",
    "    elif file_type == \"md\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return markdown.markdown(f.read())\n",
    "\n",
    "    elif file_type == \"txt\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "\n",
    "    elif file_type == \"json\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            return json.dumps(data, indent=2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "print(\"File processing functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "12eecb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counting and chunking functions defined!\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text, model_name=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model_name)\n",
    "        return len(enc.encode(text))\n",
    "    except:\n",
    "        # Fallback estimation\n",
    "        return int(len(text.split()) * 1.3)\n",
    "\n",
    "def chunk_text(text, chunk_size=20000, chunk_overlap=1000):\n",
    "    \"\"\"Split text into manageable chunks with adaptive chunking.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=count_tokens\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "print(\"Token counting and chunking functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "272fa238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume schema loaded successfully!\n",
      "Schema has 5 main sections\n"
     ]
    }
   ],
   "source": [
    "RESUME_SCHEMA = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-04/schema#\",\n",
    "    \"additionalProperties\": False,\n",
    "    \"definitions\": {\n",
    "        \"iso8601\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"e.g. 2014-06-29\",\n",
    "            \"pattern\": \"^([1-2][0-9]{3}-[0-1][0-9]-[0-3][0-9]|[1-2][0-9]{3}-[0-1][0-9]|[1-2][0-9]{3})$\"\n",
    "        }\n",
    "    },\n",
    "    \"properties\": {\n",
    "        \"basics\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": True,\n",
    "            \"properties\": {\n",
    "                \"name\": {\"type\": \"string\"},\n",
    "                \"label\": {\"type\": \"string\", \"description\": \"e.g. Web Developer\"},\n",
    "                \"image\": {\"type\": \"string\", \"description\": \"URL to image\"},\n",
    "                \"email\": {\"type\": \"string\", \"format\": \"email\"},\n",
    "                \"phone\": {\"type\": \"string\"},\n",
    "                \"url\": {\"type\": \"string\", \"format\": \"uri\"},\n",
    "                \"summary\": {\"type\": \"string\", \"description\": \"Write a short 2-3 sentence biography\"},\n",
    "                \"location\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"address\": {\"type\": \"string\"},\n",
    "                        \"postalCode\": {\"type\": \"string\"},\n",
    "                        \"city\": {\"type\": \"string\"},\n",
    "                        \"countryCode\": {\"type\": \"string\"},\n",
    "                        \"region\": {\"type\": \"string\"}\n",
    "                    }\n",
    "                },\n",
    "                \"profiles\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"network\": {\"type\": \"string\"},\n",
    "                            \"username\": {\"type\": \"string\"},\n",
    "                            \"url\": {\"type\": \"string\", \"format\": \"uri\"}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"work\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"description\": {\"type\": \"string\"},\n",
    "                    \"position\": {\"type\": \"string\"},\n",
    "                    \"url\": {\"type\": \"string\", \"format\": \"uri\"},\n",
    "                    \"startDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"endDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"summary\": {\"type\": \"string\"},\n",
    "                    \"highlights\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"education\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"institution\": {\"type\": \"string\"},\n",
    "                    \"url\": {\"type\": \"string\", \"format\": \"uri\"},\n",
    "                    \"area\": {\"type\": \"string\"},\n",
    "                    \"studyType\": {\"type\": \"string\"},\n",
    "                    \"startDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"endDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"score\": {\"type\": \"string\"},\n",
    "                    \"courses\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"skills\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"level\": {\"type\": \"string\"},\n",
    "                    \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"projects\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\"},\n",
    "                    \"description\": {\"type\": \"string\"},\n",
    "                    \"highlights\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"keywords\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"startDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"endDate\": {\"$ref\": \"#/definitions/iso8601\"},\n",
    "                    \"url\": {\"type\": \"string\", \"format\": \"uri\"},\n",
    "                    \"roles\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                    \"entity\": {\"type\": \"string\"},\n",
    "                    \"type\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"title\": \"Resume Schema\",\n",
    "    \"type\": \"object\"\n",
    "}\n",
    "\n",
    "print(\"Resume schema loaded successfully!\")\n",
    "print(f\"Schema has {len(RESUME_SCHEMA['properties'])} main sections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "45f6b6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Pydantic models defined for structured extraction!\n",
      "LLM re-initialized with fixed tool calling capabilities!\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class BasicInfo(BaseModel):\n",
    "    \"\"\"Extract basic personal information from resume.\"\"\"\n",
    "    name: Optional[str] = Field(default=\"\", description=\"Full name of the person\")\n",
    "    email: Optional[str] = Field(default=\"\", description=\"Email address\")\n",
    "    phone: Optional[str] = Field(default=\"\", description=\"Phone number\")\n",
    "    location: Optional[str] = Field(default=\"\", description=\"Location/Address\")\n",
    "    summary: Optional[str] = Field(default=\"\", description=\"Professional summary or objective\")\n",
    "\n",
    "class WorkExperience(BaseModel):\n",
    "    \"\"\"Extract work experience information.\"\"\"\n",
    "    company_name: Optional[str] = Field(default=\"\", description=\"Name of the company\")\n",
    "    position: Optional[str] = Field(default=\"\", description=\"Job title/position\")\n",
    "    start_date: Optional[str] = Field(default=\"\", description=\"Start date in YYYY-MM format\")\n",
    "    end_date: Optional[str] = Field(default=\"\", description=\"End date in YYYY-MM format or 'Present'\")\n",
    "    description: Optional[str] = Field(default=\"\", description=\"Job description and responsibilities\")\n",
    "    highlights: List[str] = Field(default=[], description=\"Key achievements or highlights\")\n",
    "\n",
    "class Education(BaseModel):\n",
    "    \"\"\"Extract education information.\"\"\"\n",
    "    institution: Optional[str] = Field(default=\"\", description=\"Name of educational institution\")\n",
    "    degree: Optional[str] = Field(default=\"\", description=\"Degree or study type\")\n",
    "    field_of_study: Optional[str] = Field(default=\"\", description=\"Field of study or area\")\n",
    "    start_date: Optional[str] = Field(default=\"\", description=\"Start date in YYYY format\")\n",
    "    end_date: Optional[str] = Field(default=\"\", description=\"End date in YYYY format\")\n",
    "    score: Optional[str] = Field(default=\"\", description=\"GPA or score if mentioned\")\n",
    "\n",
    "class Skills(BaseModel):\n",
    "    \"\"\"Extract skills information.\"\"\"\n",
    "    category: Optional[str] = Field(default=\"\", description=\"Skill category (e.g., Technical, Programming)\")\n",
    "    skills_list: List[str] = Field(default=[], description=\"List of skills in this category\")\n",
    "\n",
    "class Projects(BaseModel):\n",
    "    \"\"\"Extract project information.\"\"\"\n",
    "    name: Optional[str] = Field(default=\"\", description=\"Project name\")\n",
    "    description: Optional[str] = Field(default=\"\", description=\"Project description\")\n",
    "    technologies: List[str] = Field(default=[], description=\"Technologies used\")\n",
    "    highlights: List[str] = Field(default=[], description=\"Key achievements or features\")\n",
    "    start_date: Optional[str] = Field(default=\"\", description=\"Start date\")\n",
    "    end_date: Optional[str] = Field(default=\"\", description=\"End date\")\n",
    "\n",
    "tools = [BasicInfo, WorkExperience, Education, Skills, Projects]\n",
    "\n",
    "print(\"Fixed Pydantic models defined for structured extraction!\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "print(\"LLM re-initialized with fixed tool calling capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "70f337ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized with tool calling capabilities!\n"
     ]
    }
   ],
   "source": [
    "llm = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"LLM initialized with tool calling capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8b3b1c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized with tool calling capabilities!\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"LLM initialized with tool calling capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "99585641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Complexity Analysis:\n",
      "- Nested objects: 8\n",
      "- Max nesting level: 7\n",
      "- Schema token count: 850\n",
      "- Complexity: complex\n",
      "- Recommended chunks: 3\n"
     ]
    }
   ],
   "source": [
    "def analyze_schema_complexity(schema: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze JSON schema complexity to determine processing strategy.\"\"\"\n",
    "    \n",
    "    def count_nested_objects(obj, level=0):\n",
    "        count = 0\n",
    "        max_depth = level\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            if obj.get('type') == 'object':\n",
    "                count += 1\n",
    "            \n",
    "            for key, value in obj.items():\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    nested_count, nested_depth = count_nested_objects(value, level + 1)\n",
    "                    count += nested_count\n",
    "                    max_depth = max(max_depth, nested_depth)\n",
    "        \n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                if isinstance(item, (dict, list)):\n",
    "                    nested_count, nested_depth = count_nested_objects(item, level + 1)\n",
    "                    count += nested_count\n",
    "                    max_depth = max(max_depth, nested_depth)\n",
    "        \n",
    "        return count, max_depth\n",
    "    \n",
    "    nested_objects, max_nesting_level = count_nested_objects(schema)\n",
    "    schema_str = json.dumps(schema)\n",
    "    schema_token_count = count_tokens(schema_str)\n",
    "    \n",
    "    if nested_objects < 20 and max_nesting_level < 4:\n",
    "        complexity = \"simple\"\n",
    "        recommended_chunks = 1\n",
    "    elif nested_objects < 50 and max_nesting_level < 6:\n",
    "        complexity = \"medium\"\n",
    "        recommended_chunks = 2\n",
    "    else:\n",
    "        complexity = \"complex\"\n",
    "        recommended_chunks = 3\n",
    "    \n",
    "    return {\n",
    "        \"nested_objects\": nested_objects,\n",
    "        \"max_nesting_level\": max_nesting_level,\n",
    "        \"schema_token_count\": schema_token_count,\n",
    "        \"complexity\": complexity,\n",
    "        \"recommended_chunks\": recommended_chunks\n",
    "    }\n",
    "\n",
    "complexity_analysis = analyze_schema_complexity(RESUME_SCHEMA)\n",
    "print(\"Schema Complexity Analysis:\")\n",
    "print(f\"- Nested objects: {complexity_analysis['nested_objects']}\")\n",
    "print(f\"- Max nesting level: {complexity_analysis['max_nesting_level']}\")\n",
    "print(f\"- Schema token count: {complexity_analysis['schema_token_count']}\")\n",
    "print(f\"- Complexity: {complexity_analysis['complexity']}\")\n",
    "print(f\"- Recommended chunks: {complexity_analysis['recommended_chunks']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "09c2ebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved conversion function defined!\n"
     ]
    }
   ],
   "source": [
    "def convert_text_to_json(text: str, schema: Dict[str, Any], confidence_threshold: float = 0.7):\n",
    "    \"\"\"\n",
    "    Convert unstructured text to structured JSON following the given schema.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to convert\n",
    "        schema: JSON schema to follow\n",
    "        confidence_threshold: Minimum confidence for field acceptance\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing the structured data and confidence scores\n",
    "    \"\"\"\n",
    "    \n",
    "    complexity_analysis = analyze_schema_complexity(schema)\n",
    "    print(f\"Processing with {complexity_analysis['complexity']} complexity...\")\n",
    "    \n",
    "    text_tokens = count_tokens(text)\n",
    "    print(f\"Input text tokens: {text_tokens}\")\n",
    "    \n",
    "    max_context = 25000  \n",
    "    \n",
    "    if text_tokens > max_context:\n",
    "        print(f\"Text too large ({text_tokens} tokens), chunking into smaller parts...\")\n",
    "        chunks = chunk_text(text, chunk_size=15000, chunk_overlap=500)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "    else:\n",
    "        chunks = [text]\n",
    "        print(\"Processing as single chunk\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\nProcessing chunk {i+1}/{len(chunks)}...\")\n",
    "        \n",
    "        extraction_prompt = f\"\"\"\n",
    "        You are an expert at extracting structured information from unstructured text.\n",
    "        \n",
    "        Extract information from the following text and structure it according to the provided tools.\n",
    "        \n",
    "        IMPORTANT INSTRUCTIONS:\n",
    "        1. Extract ALL relevant information you can find\n",
    "        2. Use empty string (\"\") for missing text fields, not null\n",
    "        3. Use empty array ([]) for missing list fields, not null\n",
    "        4. Format dates as YYYY-MM-DD, YYYY-MM, or YYYY when available\n",
    "        5. If information is not available in the text, provide empty string/array as appropriate\n",
    "        6. Be thorough and accurate - extract everything you can see\n",
    "        7. For work experience, extract company name, position, dates, and all bullet points\n",
    "        8. For education, extract institution, degree, field of study, and scores\n",
    "        9. For projects, extract name, description, technologies, and achievements\n",
    "        10. For skills, group them into logical categories\n",
    "        \n",
    "        Text to extract from:\n",
    "        {chunk}\n",
    "        \n",
    "        Use the provided tools to extract information systematically. Call each tool type multiple times if you find multiple instances (e.g., multiple work experiences, education entries, etc.).\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [HumanMessage(extraction_prompt)]\n",
    "        \n",
    "        try:\n",
    "            ai_response = llm_with_tools.invoke(messages)\n",
    "            \n",
    "            if hasattr(ai_response, 'tool_calls') and ai_response.tool_calls:\n",
    "                print(f\"Found {len(ai_response.tool_calls)} tool calls\")\n",
    "                chunk_results = {}\n",
    "                \n",
    "                for tool_call in ai_response.tool_calls:\n",
    "                    tool_name = tool_call['name']\n",
    "                    tool_args = tool_call['args']\n",
    "                    print(f\"Extracted {tool_name}: {len(str(tool_args))} characters\")\n",
    "                    \n",
    "                    if tool_name not in chunk_results:\n",
    "                        chunk_results[tool_name] = []\n",
    "                    chunk_results[tool_name].append(tool_args)\n",
    "                \n",
    "                all_results.append(chunk_results)\n",
    "            else:\n",
    "                print(\"No structured data extracted from this chunk\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {i+1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if all_results:\n",
    "        final_result = merge_chunk_results(all_results)\n",
    "        return final_result\n",
    "    else:\n",
    "        print(\"No results extracted from any chunks\")\n",
    "        return {\n",
    "            \"basics\": {},\n",
    "            \"work\": [],\n",
    "            \"education\": [],\n",
    "            \"skills\": [],\n",
    "            \"projects\": []\n",
    "        }\n",
    "\n",
    "def merge_chunk_results(chunk_results: List[Dict]) -> Dict:\n",
    "    \"\"\"Merge results from multiple chunks into final structured format.\"\"\"\n",
    "    \n",
    "    merged = {\n",
    "        \"basics\": {},\n",
    "        \"work\": [],\n",
    "        \"education\": [],\n",
    "        \"skills\": [],\n",
    "        \"projects\": []\n",
    "    }\n",
    "    \n",
    "    for chunk_result in chunk_results:\n",
    "        if \"BasicInfo\" in chunk_result:\n",
    "            for basic_info in chunk_result[\"BasicInfo\"]:\n",
    "                for key, value in basic_info.items():\n",
    "                    if value and str(value).strip():  \n",
    "                        merged[\"basics\"][key] = value\n",
    "        \n",
    "        if \"WorkExperience\" in chunk_result:\n",
    "            for work in chunk_result[\"WorkExperience\"]:\n",
    "                merged[\"work\"].append({\n",
    "                    \"name\": work.get(\"company_name\", \"\"),\n",
    "                    \"position\": work.get(\"position\", \"\"),\n",
    "                    \"startDate\": work.get(\"start_date\", \"\"),\n",
    "                    \"endDate\": work.get(\"end_date\", \"\"),\n",
    "                    \"summary\": work.get(\"description\", \"\"),\n",
    "                    \"highlights\": work.get(\"highlights\", [])\n",
    "                })\n",
    "        \n",
    "        if \"Education\" in chunk_result:\n",
    "            for edu in chunk_result[\"Education\"]:\n",
    "                merged[\"education\"].append({\n",
    "                    \"institution\": edu.get(\"institution\", \"\"),\n",
    "                    \"studyType\": edu.get(\"degree\", \"\"),\n",
    "                    \"area\": edu.get(\"field_of_study\", \"\"),\n",
    "                    \"startDate\": edu.get(\"start_date\", \"\"),\n",
    "                    \"endDate\": edu.get(\"end_date\", \"\"),\n",
    "                    \"score\": edu.get(\"score\", \"\")\n",
    "                })\n",
    "        \n",
    "        if \"Skills\" in chunk_result:\n",
    "            for skill in chunk_result[\"Skills\"]:\n",
    "                merged[\"skills\"].append({\n",
    "                    \"name\": skill.get(\"category\", \"\"),\n",
    "                    \"keywords\": skill.get(\"skills_list\", [])\n",
    "                })\n",
    "        \n",
    "        if \"Projects\" in chunk_result:\n",
    "            for project in chunk_result[\"Projects\"]:\n",
    "                merged[\"projects\"].append({\n",
    "                    \"name\": project.get(\"name\", \"\"),\n",
    "                    \"description\": project.get(\"description\", \"\"),\n",
    "                    \"keywords\": project.get(\"technologies\", []),\n",
    "                    \"highlights\": project.get(\"highlights\", []),\n",
    "                    \"startDate\": project.get(\"start_date\", \"\"),\n",
    "                    \"endDate\": project.get(\"end_date\", \"\")\n",
    "                })\n",
    "    \n",
    "    return merged\n",
    "\n",
    "print(\"Improved conversion function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f2a38636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume_file(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete pipeline to process a resume file and convert to structured JSON.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the resume file\n",
    "        \n",
    "    Returns:\n",
    "        Structured JSON data following the resume schema\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Processing resume: {file_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        file_type = identify_file_type(file_path)\n",
    "        text = parse_file(file_path, file_type)\n",
    "        \n",
    "        print(f\"   - File type: {file_type}\")\n",
    "        print(f\"   - Text length: {len(text)} characters\")\n",
    "        print(f\"   - Token count: {count_tokens(text)}\")\n",
    "        \n",
    "        structured_data = convert_text_to_json(text, RESUME_SCHEMA)\n",
    "        \n",
    "        print(\"\\nConversion completed!\")\n",
    "        \n",
    "        required_sections = [\"basics\", \"work\", \"education\", \"skills\", \"projects\"]\n",
    "        \n",
    "        validation_results = {}\n",
    "        for section in required_sections:\n",
    "            if section in structured_data and structured_data[section]:\n",
    "                validation_results[section] = \"✅ Present\"\n",
    "            else:\n",
    "                validation_results[section] = \"⚠️ Missing or empty\"\n",
    "        \n",
    "        print(\"Validation Results:\")\n",
    "        for section, status in validation_results.items():\n",
    "            print(f\"   - {section}: {status}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"data\": structured_data,\n",
    "            \"validation\": validation_results,\n",
    "            \"metadata\": {\n",
    "                \"file_type\": file_type,\n",
    "                \"original_length\": len(text),\n",
    "                \"token_count\": count_tokens(text),\n",
    "                \"processing_timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file: {str(e)}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"data\": None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "df8c2e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing resume: C:\\Users\\Admin\\OneDrive\\Desktop\\JankiGabaniResume_.pdf\n",
      "==================================================\n",
      "   - File type: pdf\n",
      "   - Text length: 4587 characters\n",
      "   - Token count: 996\n",
      "Processing with complex complexity...\n",
      "Input text tokens: 996\n",
      "Processing as single chunk\n",
      "\n",
      "Processing chunk 1/1...\n",
      "Found 1 tool calls\n",
      "Extracted BasicInfo: 88 characters\n",
      "\n",
      "Conversion completed!\n",
      "Validation Results:\n",
      "   - basics: ✅ Present\n",
      "   - work: ⚠️ Missing or empty\n",
      "   - education: ⚠️ Missing or empty\n",
      "   - skills: ⚠️ Missing or empty\n",
      "   - projects: ⚠️ Missing or empty\n",
      "\n",
      " SUCCESS! Resume processed successfully!\n",
      "\n",
      "EXTRACTED DATA:\n",
      "==================================================\n",
      "{\n",
      "  \"basics\": {\n",
      "    \"name\": \"Janki Gabani\",\n",
      "    \"phone\": \"+91 96381 83121\",\n",
      "    \"email\": \"jankigabani10@gmail.com\"\n",
      "  },\n",
      "  \"work\": [],\n",
      "  \"education\": [],\n",
      "  \"skills\": [],\n",
      "  \"projects\": []\n",
      "}\n",
      "\n",
      "METADATA:\n",
      "   - File type: pdf\n",
      "   - Original length: 4587 chars\n",
      "   - Token count: 996\n",
      "   - Processing time: 2025-07-21T07:12:40.095197\n"
     ]
    }
   ],
   "source": [
    "resume_file_path = \"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\JankiGabaniResume_.pdf\"\n",
    "# resume_file_path = \"data\\\\deedy-resume-reversed.pdf\"\n",
    "\n",
    "result = process_resume_file(resume_file_path)\n",
    "\n",
    "if result[\"status\"] == \"success\":\n",
    "    print(\"\\n SUCCESS! Resume processed successfully!\")\n",
    "    print(\"\\nEXTRACTED DATA:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    structured_json = json.dumps(result[\"data\"], indent=2, ensure_ascii=False)\n",
    "    print(structured_json)\n",
    "    \n",
    "    print(f\"\\nMETADATA:\")\n",
    "    print(f\"   - File type: {result['metadata']['file_type']}\")\n",
    "    print(f\"   - Original length: {result['metadata']['original_length']} chars\")\n",
    "    print(f\"   - Token count: {result['metadata']['token_count']}\")\n",
    "    print(f\"   - Processing time: {result['metadata']['processing_timestamp']}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nFAILED to process resume:\")\n",
    "    print(f\"   Error: {result['error']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "176e1512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: extracted_resume.json\n",
      "Full results with metadata saved to: extraction_metadata.json\n"
     ]
    }
   ],
   "source": [
    "if result[\"status\"] == \"success\":\n",
    "    output_filename = \"extracted_resume.json\"\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result[\"data\"], f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Results saved to: {output_filename}\")\n",
    "    \n",
    "    metadata_filename = \"extraction_metadata.json\"\n",
    "    with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Full results with metadata saved to: {metadata_filename}\")\n",
    "else:\n",
    "    print(\"No results to save due to processing error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "906fc78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Action schema loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GITHUB_ACTION_SCHEMA = {\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"$id\": \"https://json.schemastore.org/github-action.json\",\n",
    "  \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions\",\n",
    "  \"additionalProperties\": False,\n",
    "  \"definitions\": {\n",
    "    \"expressionSyntax\": {\n",
    "      \"$comment\": \"escape `{` and `}` in pattern to be unicode compatible (#1360)\",\n",
    "      \"type\": \"string\",\n",
    "      \"pattern\": \"^\\\\$\\\\{\\\\{(.|[\\r\\n])*\\\\}\\\\}$\"\n",
    "    },\n",
    "    \"stringContainingExpressionSyntax\": {\n",
    "      \"$comment\": \"escape `{` and `}` in pattern to be unicode compatible (#1360)\",\n",
    "      \"type\": \"string\",\n",
    "      \"pattern\": \"^.*\\\\$\\\\{\\\\{(.|[\\r\\n])*\\\\}\\\\}.*$\"\n",
    "    },\n",
    "    \"pre-if\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runspre-if\",\n",
    "      \"description\": \"Allows you to define conditions for the `pre:` action execution. The `pre:` action will only run if the conditions in `pre-if` are met. If not set, then `pre-if` defaults to `always()`. Note that the `step` context is unavailable, as no steps have run yet.\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"post-if\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runspost-if\",\n",
    "      \"description\": \"Allows you to define conditions for the `post:` action execution. The `post:` action will only run if the conditions in `post-if` are met. If not set, then `post-if` defaults to `always()`.\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"runs-javascript\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runs-for-javascript-actions\",\n",
    "      \"description\": \"Configures the path to the action's code and the application used to execute the code.\",\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"using\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsusing\",\n",
    "          \"description\": \"The application used to execute the code specified in `main`.\",\n",
    "          \"enum\": [\"node12\", \"node16\", \"node20\"]\n",
    "        },\n",
    "        \"main\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsmain\",\n",
    "          \"description\": \"The file that contains your action code. The application specified in `using` executes this file.\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"pre\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runspre\",\n",
    "          \"description\": \"Allows you to run a script at the start of a job, before the `main:` action begins. For example, you can use `pre:` to run a prerequisite setup script. The application specified with the `using` syntax will execute this file. The `pre:` action always runs by default but you can override this using `pre-if`.\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"pre-if\": {\n",
    "          \"$ref\": \"#/definitions/pre-if\"\n",
    "        },\n",
    "        \"post\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runspost\",\n",
    "          \"description\": \"Allows you to run a script at the end of a job, once the `main:` action has completed. For example, you can use `post:` to terminate certain processes or remove unneeded files. The application specified with the `using` syntax will execute this file. The `post:` action always runs by default but you can override this using `post-if`.\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"post-if\": {\n",
    "          \"$ref\": \"#/definitions/post-if\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"using\", \"main\"],\n",
    "      \"additionalProperties\": False\n",
    "    },\n",
    "    \"runs-composite\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runs-for-composite-actions\",\n",
    "      \"description\": \"Configures the path to the composite action, and the application used to execute the code.\",\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"using\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsusing-for-composite-actions\",\n",
    "          \"description\": \"To use a composite run steps action, set this to 'composite'.\",\n",
    "          \"const\": \"composite\"\n",
    "        },\n",
    "        \"steps\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runssteps\",\n",
    "          \"description\": \"The run steps that you plan to run in this action.\",\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"run\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepsrun\",\n",
    "                \"description\": \"The command you want to run. This can be inline or a script in your action repository.\",\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"shell\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepsshell\",\n",
    "                \"description\": \"The shell where you want to run the command.\",\n",
    "                \"type\": \"string\",\n",
    "                \"anyOf\": [\n",
    "                  {\n",
    "                    \"$comment\": \"https://help.github.com/en/actions/reference/workflow-syntax-for-github-actions#custom-shell\"\n",
    "                  },\n",
    "                  {\n",
    "                    \"enum\": [\n",
    "                      \"bash\",\n",
    "                      \"pwsh\",\n",
    "                      \"python\",\n",
    "                      \"sh\",\n",
    "                      \"cmd\",\n",
    "                      \"powershell\"\n",
    "                    ]\n",
    "                  }\n",
    "                ]\n",
    "              },\n",
    "              \"uses\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepsuses\",\n",
    "                \"description\": \"Selects an action to run as part of a step in your job.\",\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"with\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepswith\",\n",
    "                \"description\": \"A map of the input parameters defined by the action. Each input parameter is a key/value pair. Input parameters are set as environment variables. The variable is prefixed with INPUT_ and converted to upper case.\",\n",
    "                \"type\": \"object\"\n",
    "              },\n",
    "              \"name\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepsname\",\n",
    "                \"description\": \"The name of the composite run step.\",\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"id\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepsid\",\n",
    "                \"description\": \"A unique identifier for the step. You can use the `id` to reference the step in contexts.\",\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"if\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepsif\",\n",
    "                \"description\": \"You can use the if conditional to prevent a step from running unless a condition is met. You can use any supported context and expression to create a conditional.\\nExpressions in an if conditional do not require the ${{ }} syntax. For more information, see https://help.github.com/en/articles/contexts-and-expression-syntax-for-github-actions.\",\n",
    "                \"type\": \"string\"\n",
    "              },\n",
    "              \"env\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepsenv\",\n",
    "                \"description\": \"Sets a map of environment variables for only that step.\",\n",
    "                \"oneOf\": [\n",
    "                  {\n",
    "                    \"type\": \"object\",\n",
    "                    \"additionalProperties\": {\n",
    "                      \"oneOf\": [\n",
    "                        {\n",
    "                          \"type\": \"string\"\n",
    "                        },\n",
    "                        {\n",
    "                          \"type\": \"number\"\n",
    "                        },\n",
    "                        {\n",
    "                          \"type\": \"boolean\"\n",
    "                        }\n",
    "                      ]\n",
    "                    }\n",
    "                  },\n",
    "                  {\n",
    "                    \"$ref\": \"#/definitions/stringContainingExpressionSyntax\"\n",
    "                  }\n",
    "                ]\n",
    "              },\n",
    "              \"continue-on-error\": {\n",
    "                \"$comment\": \"https://help.github.com/en/actions/automating-your-workflow-with-github-actions/workflow-syntax-for-github-actions#jobsjob_idstepscontinue-on-error\",\n",
    "                \"description\": \"Prevents a job from failing when a step fails. Set to true to allow a job to pass when this step fails.\",\n",
    "                \"oneOf\": [\n",
    "                  {\n",
    "                    \"type\": \"boolean\"\n",
    "                  },\n",
    "                  {\n",
    "                    \"$ref\": \"#/definitions/expressionSyntax\"\n",
    "                  }\n",
    "                ],\n",
    "                \"default\": False\n",
    "              },\n",
    "              \"working-directory\": {\n",
    "                \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsstepsworking-directory\",\n",
    "                \"description\": \"Specifies the working directory where the command is run.\",\n",
    "                \"type\": \"string\"\n",
    "              }\n",
    "            },\n",
    "            \"oneOf\": [\n",
    "              {\n",
    "                \"required\": [\"run\", \"shell\"]\n",
    "              },\n",
    "              {\n",
    "                \"required\": [\"uses\"]\n",
    "              }\n",
    "            ],\n",
    "            \"additionalProperties\": False\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"using\", \"steps\"],\n",
    "      \"additionalProperties\": False\n",
    "    },\n",
    "    \"runs-docker\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runs-for-docker-container-actions\",\n",
    "      \"description\": \"Configures the image used for the Docker action.\",\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"using\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsusing-for-docker-container-actions\",\n",
    "          \"description\": \"You must set this value to 'docker'.\",\n",
    "          \"const\": \"docker\"\n",
    "        },\n",
    "        \"image\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsimage\",\n",
    "          \"description\": \"The Docker image to use as the container to run the action. The value can be the Docker base image name, a local `Dockerfile` in your repository, or a public image in Docker Hub or another registry. To reference a `Dockerfile` local to your repository, use a path relative to your action metadata file. The `docker` application will execute this file.\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"env\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsenv\",\n",
    "          \"description\": \"Specifies a key/value map of environment variables to set in the container environment.\",\n",
    "          \"oneOf\": [\n",
    "            {\n",
    "              \"type\": \"object\",\n",
    "              \"additionalProperties\": {\n",
    "                \"oneOf\": [\n",
    "                  {\n",
    "                    \"type\": \"string\"\n",
    "                  },\n",
    "                  {\n",
    "                    \"type\": \"number\"\n",
    "                  },\n",
    "                  {\n",
    "                    \"type\": \"boolean\"\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"$ref\": \"#/definitions/stringContainingExpressionSyntax\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        \"entrypoint\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsentrypoint\",\n",
    "          \"description\": \"Overrides the Docker `ENTRYPOINT` in the `Dockerfile`, or sets it if one wasn't already specified. Use `entrypoint` when the `Dockerfile` does not specify an `ENTRYPOINT` or you want to override the `ENTRYPOINT` instruction. If you omit `entrypoint`, the commands you specify in the Docker `ENTRYPOINT` instruction will execute. The Docker `ENTRYPOINT instruction has a *shell* form and *exec* form. The Docker `ENTRYPOINT` documentation recommends using the *exec* form of the `ENTRYPOINT` instruction.\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"pre-entrypoint\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runspre-entrypoint\",\n",
    "          \"description\": \"Allows you to run a script before the `entrypoint` action begins. For example, you can use `pre-entrypoint:` to run a prerequisite setup script. GitHub Actions uses `docker run` to launch this action, and runs the script inside a new container that uses the same base image. This means that the runtime state is different from the main `entrypoint` container, and any states you require must be accessed in either the workspace, `HOME`, or as a `STATE_` variable. The `pre-entrypoint:` action always runs by default but you can override this using `pre-if`.\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"pre-if\": {\n",
    "          \"$ref\": \"#/definitions/pre-if\"\n",
    "        },\n",
    "        \"post-entrypoint\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runspost-entrypoint\",\n",
    "          \"description\": \"Allows you to run a cleanup script once the `runs.entrypoint` action has completed. GitHub Actions uses `docker run` to launch this action. Because GitHub Actions runs the script inside a new container using the same base image, the runtime state is different from the main `entrypoint` container. You can access any state you need in either the workspace, `HOME`, or as a `STATE_` variable. The `post-entrypoint:` action always runs by default but you can override this using `post-if`.\",\n",
    "          \"type\": \"string\"\n",
    "        },\n",
    "        \"post-if\": {\n",
    "          \"$ref\": \"#/definitions/post-if\"\n",
    "        },\n",
    "        \"args\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#runsargs\",\n",
    "          \"description\": \"An array of strings that define the inputs for a Docker container. Inputs can include hardcoded strings. GitHub passes the `args` to the container's `ENTRYPOINT` when the container starts up.\\nThe `args` are used in place of the `CMD` instruction in a `Dockerfile`. If you use `CMD` in your `Dockerfile`, use the guidelines ordered by preference:\\n- Document required arguments in the action's README and omit them from the `CMD` instruction.\\n- Use defaults that allow using the action without specifying any `args`.\\n- If the action exposes a `--help` flag, or something similar, use that to make your action self-documenting.\",\n",
    "          \"type\": \"array\",\n",
    "          \"items\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\"using\", \"image\"],\n",
    "      \"additionalProperties\": False\n",
    "    },\n",
    "    \"outputs\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#outputs-for-docker-container-and-javascript-actions\",\n",
    "      \"description\": \"Output parameters allow you to declare data that an action sets. Actions that run later in a workflow can use the output data set in previously run actions. For example, if you had an action that performed the addition of two inputs (x + y = z), the action could output the sum (z) for other actions to use as an input.\\nIf you don't declare an output in your action metadata file, you can still set outputs and use them in a workflow.\",\n",
    "      \"type\": \"object\",\n",
    "      \"patternProperties\": {\n",
    "        \"^[_a-zA-Z][a-zA-Z0-9_-]*$\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#outputsoutput_id\",\n",
    "          \"description\": \"A string identifier to associate with the output. The value of `<output_id>` is a map of the output's metadata. The `<output_id>` must be a unique identifier within the outputs object. The `<output_id>` must start with a letter or `_` and contain only alphanumeric characters, `-`, or `_`.\",\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"description\": {\n",
    "              \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#outputsoutput_iddescription\",\n",
    "              \"description\": \"A string description of the output parameter.\",\n",
    "              \"type\": \"string\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"description\"],\n",
    "          \"additionalProperties\": False\n",
    "        }\n",
    "      },\n",
    "      \"additionalProperties\": False\n",
    "    },\n",
    "    \"outputs-composite\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#outputs-for-composite-actions\",\n",
    "      \"description\": \"Output parameters allow you to declare data that an action sets. Actions that run later in a workflow can use the output data set in previously run actions. For example, if you had an action that performed the addition of two inputs (x + y = z), the action could output the sum (z) for other actions to use as an input.\\nIf you don't declare an output in your action metadata file, you can still set outputs and use them in a workflow.\",\n",
    "      \"type\": \"object\",\n",
    "      \"patternProperties\": {\n",
    "        \"^[_a-zA-Z][a-zA-Z0-9_-]*$\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#outputsoutput_id\",\n",
    "          \"description\": \"A string identifier to associate with the output. The value of `<output_id>` is a map of the output's metadata. The `<output_id>` must be a unique identifier within the outputs object. The `<output_id>` must start with a letter or `_` and contain only alphanumeric characters, `-`, or `_`.\",\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"description\": {\n",
    "              \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#outputsoutput_iddescription\",\n",
    "              \"description\": \"A string description of the output parameter.\",\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            \"value\": {\n",
    "              \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#outputsoutput_idvalue\",\n",
    "              \"description\": \"The value that the output parameter will be mapped to. You can set this to a string or an expression with context. For example, you can use the steps context to set the value of an output to the output value of a step.\",\n",
    "              \"type\": \"string\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"description\", \"value\"],\n",
    "          \"additionalProperties\": False\n",
    "        }\n",
    "      },\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  },\n",
    "  \"else\": {\n",
    "    \"properties\": {\n",
    "      \"outputs\": {\n",
    "        \"$ref\": \"#/definitions/outputs\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"if\": {\n",
    "    \"properties\": {\n",
    "      \"runs\": {\n",
    "        \"properties\": {\n",
    "          \"using\": {\n",
    "            \"const\": \"composite\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"properties\": {\n",
    "    \"name\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#name\",\n",
    "      \"description\": \"The name of your action. GitHub displays the `name` in the Actions tab to help visually identify actions in each job.\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"author\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#author\",\n",
    "      \"description\": \"The name of the action's author.\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"description\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#description\",\n",
    "      \"description\": \"A short description of the action.\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"inputs\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#inputs\",\n",
    "      \"description\": \"Input parameters allow you to specify data that the action expects to use during runtime. GitHub stores input parameters as environment variables. Input ids with uppercase letters are converted to lowercase during runtime. We recommended using lowercase input ids.\",\n",
    "      \"type\": \"object\",\n",
    "      \"patternProperties\": {\n",
    "        \"^[_a-zA-Z][a-zA-Z0-9_-]*$\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#inputsinput_id\",\n",
    "          \"description\": \"A string identifier to associate with the input. The value of `<input_id>` is a map of the input's metadata. The `<input_id>` must be a unique identifier within the inputs object. The `<input_id>` must start with a letter or `_` and contain only alphanumeric characters, `-`, or `_`.\",\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"description\": {\n",
    "              \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#inputsinput_iddescription\",\n",
    "              \"description\": \"A string description of the input parameter.\",\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            \"deprecationMessage\": {\n",
    "              \"description\": \"A string shown to users using the deprecated input.\",\n",
    "              \"type\": \"string\"\n",
    "            },\n",
    "            \"required\": {\n",
    "              \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#inputsinput_idrequired\",\n",
    "              \"description\": \"A boolean to indicate whether the action requires the input parameter. Set to `true` when the parameter is required.\",\n",
    "              \"type\": \"boolean\"\n",
    "            },\n",
    "            \"default\": {\n",
    "              \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#inputsinput_iddefault\",\n",
    "              \"description\": \"A string representing the default value. The default value is used when an input parameter isn't specified in a workflow file.\",\n",
    "              \"type\": \"string\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"description\"],\n",
    "          \"additionalProperties\": False\n",
    "        }\n",
    "      },\n",
    "      \"additionalProperties\": False\n",
    "    },\n",
    "    \"outputs\": {\n",
    "      \"$comment\": \"Because of `additionalProperties: false`, this empty schema is needed to allow the `outputs` property. The `outputs` subschema is determined by the if/then/else keywords.\"\n",
    "    },\n",
    "    \"runs\": {\n",
    "      \"oneOf\": [\n",
    "        {\n",
    "          \"$ref\": \"#/definitions/runs-javascript\"\n",
    "        },\n",
    "        {\n",
    "          \"$ref\": \"#/definitions/runs-composite\"\n",
    "        },\n",
    "        {\n",
    "          \"$ref\": \"#/definitions/runs-docker\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"branding\": {\n",
    "      \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#branding\",\n",
    "      \"description\": \"You can use a color and Feather icon to create a badge to personalize and distinguish your action. Badges are shown next to your action name in GitHub Marketplace.\",\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"color\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#brandingcolor\",\n",
    "          \"description\": \"The background color of the badge.\",\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\n",
    "            \"white\",\n",
    "            \"black\",\n",
    "            \"yellow\",\n",
    "            \"blue\",\n",
    "            \"green\",\n",
    "            \"orange\",\n",
    "            \"red\",\n",
    "            \"purple\",\n",
    "            \"gray-dark\"\n",
    "          ]\n",
    "        },\n",
    "        \"icon\": {\n",
    "          \"$comment\": \"https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions#brandingicon\",\n",
    "          \"description\": \"The name of the Feather icon to use.\",\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\n",
    "            \"activity\",\n",
    "            \"airplay\",\n",
    "            \"alert-circle\",\n",
    "            \"alert-octagon\",\n",
    "            \"alert-triangle\",\n",
    "            \"align-center\",\n",
    "            \"align-justify\",\n",
    "            \"align-left\",\n",
    "            \"align-right\",\n",
    "            \"anchor\",\n",
    "            \"aperture\",\n",
    "            \"archive\",\n",
    "            \"arrow-down-circle\",\n",
    "            \"arrow-down-left\",\n",
    "            \"arrow-down-right\",\n",
    "            \"arrow-down\",\n",
    "            \"arrow-left-circle\",\n",
    "            \"arrow-left\",\n",
    "            \"arrow-right-circle\",\n",
    "            \"arrow-right\",\n",
    "            \"arrow-up-circle\",\n",
    "            \"arrow-up-left\",\n",
    "            \"arrow-up-right\",\n",
    "            \"arrow-up\",\n",
    "            \"at-sign\",\n",
    "            \"award\",\n",
    "            \"bar-chart-2\",\n",
    "            \"bar-chart\",\n",
    "            \"battery-charging\",\n",
    "            \"battery\",\n",
    "            \"bell-off\",\n",
    "            \"bell\",\n",
    "            \"bluetooth\",\n",
    "            \"bold\",\n",
    "            \"book-open\",\n",
    "            \"book\",\n",
    "            \"bookmark\",\n",
    "            \"box\",\n",
    "            \"briefcase\",\n",
    "            \"calendar\",\n",
    "            \"camera-off\",\n",
    "            \"camera\",\n",
    "            \"cast\",\n",
    "            \"check-circle\",\n",
    "            \"check-square\",\n",
    "            \"check\",\n",
    "            \"chevron-down\",\n",
    "            \"chevron-left\",\n",
    "            \"chevron-right\",\n",
    "            \"chevron-up\",\n",
    "            \"chevrons-down\",\n",
    "            \"chevrons-left\",\n",
    "            \"chevrons-right\",\n",
    "            \"chevrons-up\",\n",
    "            \"circle\",\n",
    "            \"clipboard\",\n",
    "            \"clock\",\n",
    "            \"cloud-drizzle\",\n",
    "            \"cloud-lightning\",\n",
    "            \"cloud-off\",\n",
    "            \"cloud-rain\",\n",
    "            \"cloud-snow\",\n",
    "            \"cloud\",\n",
    "            \"code\",\n",
    "            \"command\",\n",
    "            \"compass\",\n",
    "            \"copy\",\n",
    "            \"corner-down-left\",\n",
    "            \"corner-down-right\",\n",
    "            \"corner-left-down\",\n",
    "            \"corner-left-up\",\n",
    "            \"corner-right-down\",\n",
    "            \"corner-right-up\",\n",
    "            \"corner-up-left\",\n",
    "            \"corner-up-right\",\n",
    "            \"cpu\",\n",
    "            \"credit-card\",\n",
    "            \"crop\",\n",
    "            \"crosshair\",\n",
    "            \"database\",\n",
    "            \"delete\",\n",
    "            \"disc\",\n",
    "            \"dollar-sign\",\n",
    "            \"download-cloud\",\n",
    "            \"download\",\n",
    "            \"droplet\",\n",
    "            \"edit-2\",\n",
    "            \"edit-3\",\n",
    "            \"edit\",\n",
    "            \"external-link\",\n",
    "            \"eye-off\",\n",
    "            \"eye\",\n",
    "            \"fast-forward\",\n",
    "            \"feather\",\n",
    "            \"file-minus\",\n",
    "            \"file-plus\",\n",
    "            \"file-text\",\n",
    "            \"file\",\n",
    "            \"film\",\n",
    "            \"filter\",\n",
    "            \"flag\",\n",
    "            \"folder-minus\",\n",
    "            \"folder-plus\",\n",
    "            \"folder\",\n",
    "            \"gift\",\n",
    "            \"git-branch\",\n",
    "            \"git-commit\",\n",
    "            \"git-merge\",\n",
    "            \"git-pull-request\",\n",
    "            \"globe\",\n",
    "            \"grid\",\n",
    "            \"hard-drive\",\n",
    "            \"hash\",\n",
    "            \"headphones\",\n",
    "            \"heart\",\n",
    "            \"help-circle\",\n",
    "            \"home\",\n",
    "            \"image\",\n",
    "            \"inbox\",\n",
    "            \"info\",\n",
    "            \"italic\",\n",
    "            \"layers\",\n",
    "            \"layout\",\n",
    "            \"life-buoy\",\n",
    "            \"link-2\",\n",
    "            \"link\",\n",
    "            \"list\",\n",
    "            \"loader\",\n",
    "            \"lock\",\n",
    "            \"log-in\",\n",
    "            \"log-out\",\n",
    "            \"mail\",\n",
    "            \"map-pin\",\n",
    "            \"map\",\n",
    "            \"maximize-2\",\n",
    "            \"maximize\",\n",
    "            \"menu\",\n",
    "            \"message-circle\",\n",
    "            \"message-square\",\n",
    "            \"mic-off\",\n",
    "            \"mic\",\n",
    "            \"minimize-2\",\n",
    "            \"minimize\",\n",
    "            \"minus-circle\",\n",
    "            \"minus-square\",\n",
    "            \"minus\",\n",
    "            \"monitor\",\n",
    "            \"moon\",\n",
    "            \"more-horizontal\",\n",
    "            \"more-vertical\",\n",
    "            \"move\",\n",
    "            \"music\",\n",
    "            \"navigation-2\",\n",
    "            \"navigation\",\n",
    "            \"octagon\",\n",
    "            \"package\",\n",
    "            \"paperclip\",\n",
    "            \"pause-circle\",\n",
    "            \"pause\",\n",
    "            \"percent\",\n",
    "            \"phone-call\",\n",
    "            \"phone-forwarded\",\n",
    "            \"phone-incoming\",\n",
    "            \"phone-missed\",\n",
    "            \"phone-off\",\n",
    "            \"phone-outgoing\",\n",
    "            \"phone\",\n",
    "            \"pie-chart\",\n",
    "            \"play-circle\",\n",
    "            \"play\",\n",
    "            \"plus-circle\",\n",
    "            \"plus-square\",\n",
    "            \"plus\",\n",
    "            \"pocket\",\n",
    "            \"power\",\n",
    "            \"printer\",\n",
    "            \"radio\",\n",
    "            \"refresh-ccw\",\n",
    "            \"refresh-cw\",\n",
    "            \"repeat\",\n",
    "            \"rewind\",\n",
    "            \"rotate-ccw\",\n",
    "            \"rotate-cw\",\n",
    "            \"rss\",\n",
    "            \"save\",\n",
    "            \"scissors\",\n",
    "            \"search\",\n",
    "            \"send\",\n",
    "            \"server\",\n",
    "            \"settings\",\n",
    "            \"share-2\",\n",
    "            \"share\",\n",
    "            \"shield-off\",\n",
    "            \"shield\",\n",
    "            \"shopping-bag\",\n",
    "            \"shopping-cart\",\n",
    "            \"shuffle\",\n",
    "            \"sidebar\",\n",
    "            \"skip-back\",\n",
    "            \"skip-forward\",\n",
    "            \"slash\",\n",
    "            \"sliders\",\n",
    "            \"smartphone\",\n",
    "            \"speaker\",\n",
    "            \"square\",\n",
    "            \"star\",\n",
    "            \"stop-circle\",\n",
    "            \"sun\",\n",
    "            \"sunrise\",\n",
    "            \"sunset\",\n",
    "            \"table\",\n",
    "            \"tablet\",\n",
    "            \"tag\",\n",
    "            \"target\",\n",
    "            \"terminal\",\n",
    "            \"thermometer\",\n",
    "            \"thumbs-down\",\n",
    "            \"thumbs-up\",\n",
    "            \"toggle-left\",\n",
    "            \"toggle-right\",\n",
    "            \"trash-2\",\n",
    "            \"trash\",\n",
    "            \"trending-down\",\n",
    "            \"trending-up\",\n",
    "            \"triangle\",\n",
    "            \"truck\",\n",
    "            \"tv\",\n",
    "            \"type\",\n",
    "            \"umbrella\",\n",
    "            \"underline\",\n",
    "            \"unlock\",\n",
    "            \"upload-cloud\",\n",
    "            \"upload\",\n",
    "            \"user-check\",\n",
    "            \"user-minus\",\n",
    "            \"user-plus\",\n",
    "            \"user-x\",\n",
    "            \"user\",\n",
    "            \"users\",\n",
    "            \"video-off\",\n",
    "            \"video\",\n",
    "            \"voicemail\",\n",
    "            \"volume-1\",\n",
    "            \"volume-2\",\n",
    "            \"volume-x\",\n",
    "            \"volume\",\n",
    "            \"watch\",\n",
    "            \"wifi-off\",\n",
    "            \"wifi\",\n",
    "            \"wind\",\n",
    "            \"x-circle\",\n",
    "            \"x-square\",\n",
    "            \"x\",\n",
    "            \"zap-off\",\n",
    "            \"zap\",\n",
    "            \"zoom-in\",\n",
    "            \"zoom-out\"\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "      \"additionalProperties\": False\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"name\", \"description\", \"runs\"],\n",
    "  \"then\": {\n",
    "    \"properties\": {\n",
    "      \"outputs\": {\n",
    "        \"$ref\": \"#/definitions/outputs-composite\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"type\": \"object\"\n",
    "}\n",
    "\n",
    "print(\"GitHub Action schema loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f8f804fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced file processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import re\n",
    "\n",
    "def identify_file_type_enhanced(file_path):\n",
    "    \"\"\"Enhanced file type identification including YAML and action files.\"\"\"\n",
    "    file_path_lower = file_path.lower()\n",
    "\n",
    "    if file_path_lower.endswith((\".yml\", \".yaml\")):\n",
    "        return \"yaml\"\n",
    "    elif file_path_lower.endswith(\".md\"):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            if \"action\" in content.lower() and (\"input\" in content.lower() or \"output\" in content.lower()):\n",
    "                return \"action_md\"\n",
    "            return \"md\"\n",
    "    elif file_path_lower.endswith(\".pdf\"):\n",
    "        return \"pdf\"\n",
    "    elif file_path_lower.endswith(\".docx\"):\n",
    "        return \"docx\"\n",
    "    elif file_path_lower.endswith(\".csv\"):\n",
    "        return \"csv\"\n",
    "    elif file_path_lower.endswith(\".txt\"):\n",
    "        return \"txt\"\n",
    "    elif file_path_lower.endswith(\".json\"):\n",
    "        return \"json\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "\n",
    "def parse_file_enhanced(file_path, file_type):\n",
    "    \"\"\"Enhanced file parsing with support for YAML and action descriptions.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    if file_type == \"yaml\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = yaml.safe_load(f)\n",
    "            return yaml.dump(data, default_flow_style=False)\n",
    "    \n",
    "    elif file_type == \"action_md\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "    \n",
    "    elif file_type == \"pdf\":\n",
    "        text = \"\"\n",
    "        with fitz.open(file_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        return text.strip()\n",
    "\n",
    "    elif file_type == \"docx\":\n",
    "        return docx2txt.process(file_path).strip()\n",
    "\n",
    "    elif file_type == \"csv\":\n",
    "        chunks = []\n",
    "        for chunk in pd.read_csv(file_path, chunksize=10000):\n",
    "            chunks.append(chunk)\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        return df.to_string(index=False)\n",
    "\n",
    "    elif file_type == \"md\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return markdown.markdown(f.read())\n",
    "\n",
    "    elif file_type == \"txt\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read().strip()\n",
    "\n",
    "    elif file_type == \"json\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            return json.dumps(data, indent=2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "print(\"Enhanced file processing functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "38aeef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Action Pydantic models defined!\n"
     ]
    }
   ],
   "source": [
    "class ActionInputOutput(BaseModel):\n",
    "    \"\"\"Extract input or output definition.\"\"\"\n",
    "    name: str = Field(description=\"Name of the input/output parameter\")\n",
    "    description: str = Field(description=\"Description of the parameter\")\n",
    "    required: Optional[bool] = Field(description=\"Whether the parameter is required\")\n",
    "    default_value: Optional[str] = Field(description=\"Default value if any\")\n",
    "\n",
    "class ActionStep(BaseModel):\n",
    "    \"\"\"Extract action step information.\"\"\"\n",
    "    id: Optional[str] = Field(description=\"Step ID\")\n",
    "    name: str = Field(description=\"Step name or description\")\n",
    "    action_type: str = Field(description=\"Type: 'uses' for external action or 'run' for command\")\n",
    "    uses: Optional[str] = Field(description=\"External action being used (e.g., actions/checkout@v4)\")\n",
    "    run_command: Optional[str] = Field(description=\"Command being executed\")\n",
    "    shell: Optional[str] = Field(description=\"Shell type (bash, powershell, etc.)\")\n",
    "    with_parameters: Optional[Dict[str, str]] = Field(description=\"Parameters passed to the action\")\n",
    "    condition: Optional[str] = Field(description=\"Conditional execution (if clause)\")\n",
    "\n",
    "class ActionBranding(BaseModel):\n",
    "    \"\"\"Extract branding information.\"\"\"\n",
    "    icon: str = Field(description=\"Icon name\")\n",
    "    color: str = Field(description=\"Color theme\")\n",
    "\n",
    "class GitHubActionInfo(BaseModel):\n",
    "    \"\"\"Extract complete GitHub Action information.\"\"\"\n",
    "    name: str = Field(description=\"Action name\")\n",
    "    author: Optional[str] = Field(description=\"Action author\")\n",
    "    description: str = Field(description=\"Action description\")\n",
    "    purpose: Optional[str] = Field(description=\"Purpose or use case of the action\")\n",
    "\n",
    "github_action_tools = [ActionInputOutput, ActionStep, ActionBranding, GitHubActionInfo]\n",
    "\n",
    "print(\"GitHub Action Pydantic models defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "37b3b0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specialized processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def determine_document_type_and_schema(file_path, file_type):\n",
    "    \"\"\"Determine document type and return appropriate schema and tools.\"\"\"\n",
    "    \n",
    "    if file_type in [\"action_md\", \"yaml\"]:\n",
    "        return \"github_action\", GITHUB_ACTION_SCHEMA, github_action_tools\n",
    "    elif file_type in [\"pdf\", \"docx\", \"txt\"] and \"resume\" in file_path.lower():\n",
    "        return \"resume\", RESUME_SCHEMA, tools\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read().lower()\n",
    "            \n",
    "        if any(keyword in content for keyword in [\"action\", \"workflow\", \"github\", \"steps\", \"inputs\", \"outputs\"]):\n",
    "            return \"github_action\", GITHUB_ACTION_SCHEMA, github_action_tools\n",
    "        elif any(keyword in content for keyword in [\"experience\", \"education\", \"skills\", \"resume\", \"cv\"]):\n",
    "            return \"resume\", RESUME_SCHEMA, tools\n",
    "        else:\n",
    "            return \"resume\", RESUME_SCHEMA, tools\n",
    "\n",
    "def merge_github_action_results(chunk_results: List[Dict]) -> Dict:\n",
    "    \"\"\"Merge GitHub Action results from multiple chunks.\"\"\"\n",
    "\n",
    "    merged = {\n",
    "        \"name\": \"\",\n",
    "        \"author\": \"\",\n",
    "        \"description\": \"\",\n",
    "        \"branding\": {\n",
    "            \"icon\": \"\",\n",
    "            \"color\": \"\"\n",
    "        },\n",
    "        \"inputs\": {},\n",
    "        \"outputs\": {},\n",
    "        \"runs\": {\n",
    "            \"using\": \"composite\",\n",
    "            \"steps\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for chunk_result in chunk_results:\n",
    "        if \"GitHubActionInfo\" in chunk_result:\n",
    "            for action_info in chunk_result[\"GitHubActionInfo\"]:\n",
    "                for key, value in action_info.items():\n",
    "                    if value and value.strip():\n",
    "                        if key == \"purpose\":\n",
    "                            # Append purpose to description if different\n",
    "                            if merged[\"description\"] and value not in merged[\"description\"]:\n",
    "                                merged[\"description\"] += f\" {value}\"\n",
    "                            elif not merged[\"description\"]:\n",
    "                                merged[\"description\"] = value\n",
    "                        else:\n",
    "                            merged[key] = value\n",
    "\n",
    "        if \"ActionBranding\" in chunk_result:\n",
    "            for branding in chunk_result[\"ActionBranding\"]:\n",
    "                merged[\"branding\"][\"icon\"] = branding.get(\"icon\", \"\")\n",
    "                merged[\"branding\"][\"color\"] = branding.get(\"color\", \"\")\n",
    "\n",
    "        if \"ActionInputOutput\" in chunk_result:\n",
    "            for param in chunk_result[\"ActionInputOutput\"]:\n",
    "                param_name = param.get(\"name\", \"\")\n",
    "                if param_name:\n",
    "                    param_data = {\n",
    "                        \"description\": param.get(\"description\", \"\"),\n",
    "                        \"required\": param.get(\"required\", False)\n",
    "                    }\n",
    "                    if param.get(\"default_value\"):\n",
    "                        param_data[\"default\"] = param.get(\"default_value\")\n",
    "\n",
    "                    if param_name == \"page-url\":\n",
    "                        merged[\"outputs\"][param_name] = {\n",
    "                            \"description\": param.get(\"description\", \"\"),\n",
    "                            \"value\": param.get(\"default_value\", \"\")\n",
    "                        }\n",
    "                    else:\n",
    "                        merged[\"inputs\"][param_name] = param_data\n",
    "\n",
    "        if \"ActionStep\" in chunk_result:\n",
    "            for step in chunk_result[\"ActionStep\"]:\n",
    "                step_data = {\n",
    "                    \"name\": step.get(\"name\", \"\")\n",
    "                }\n",
    "\n",
    "                if step.get(\"id\"):\n",
    "                    step_data[\"id\"] = step.get(\"id\")\n",
    "\n",
    "                if step.get(\"uses\"):\n",
    "                    step_data[\"uses\"] = step.get(\"uses\")\n",
    "\n",
    "                if step.get(\"run_command\"):\n",
    "                    step_data[\"run\"] = step.get(\"run_command\")\n",
    "\n",
    "                if step.get(\"shell\"):\n",
    "                    step_data[\"shell\"] = step.get(\"shell\")\n",
    "\n",
    "                if step.get(\"with_parameters\"):\n",
    "                    step_data[\"with\"] = step.get(\"with_parameters\")\n",
    "\n",
    "                if step.get(\"condition\"):\n",
    "                    step_data[\"if\"] = step.get(\"condition\")\n",
    "\n",
    "                merged[\"runs\"][\"steps\"].append(step_data)\n",
    "\n",
    "    return merged\n",
    "\n",
    "print(\"Specialized processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "249ee04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal document processor defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_document_to_json(text: str, doc_type: str, schema: Dict[str, Any], processing_tools: List, confidence_threshold: float = 0.7):\n",
    "    \"\"\"\n",
    "    Universal function to convert any document type to structured JSON.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to convert\n",
    "        doc_type: Type of document (resume, github_action, etc.)\n",
    "        schema: JSON schema to follow\n",
    "        processing_tools: Pydantic tools for extraction\n",
    "        confidence_threshold: Minimum confidence for field acceptance\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the structured data\n",
    "    \"\"\"\n",
    "\n",
    "    complexity_analysis = analyze_schema_complexity(schema)\n",
    "    print(f\"Processing {doc_type} with {complexity_analysis['complexity']} complexity...\")\n",
    "    text_tokens = count_tokens(text)\n",
    "    print(f\"Input text tokens: {text_tokens}\")\n",
    "\n",
    "    max_context = 25000  # Conservative limit\n",
    "\n",
    "    if text_tokens > max_context:\n",
    "        print(f\"Text too large ({text_tokens} tokens), chunking into smaller parts...\")\n",
    "        chunks = chunk_text(text, chunk_size=15000, chunk_overlap=500)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "    else:\n",
    "        chunks = [text]\n",
    "        print(\"Processing as single chunk\")\n",
    "\n",
    "    llm_with_doc_tools = llm.bind_tools(processing_tools)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\nProcessing chunk {i+1}/{len(chunks)}...\")\n",
    "\n",
    "        if doc_type == \"github_action\":\n",
    "            extraction_prompt = f\"\"\"\n",
    "            You are an expert at extracting GitHub Action workflow information from text descriptions.\n",
    "\n",
    "            Extract information from the following text and structure it according to the GitHub Action schema.\n",
    "\n",
    "            IMPORTANT INSTRUCTIONS:\n",
    "            1. Extract action name, description, and purpose\n",
    "            2. Identify all inputs with their descriptions, requirements, and defaults\n",
    "            3. Identify all outputs with their descriptions and values, including the page-url\n",
    "            4. Extract all workflow steps with their actions, commands, and parameters\n",
    "            5. Look for branding information (icon, color)\n",
    "            6. Format step conditions and shell specifications correctly\n",
    "\n",
    "            Schema to follow:\n",
    "            {json.dumps(schema, indent=2)}\n",
    "\n",
    "            Text to extract from:\n",
    "            {chunk}\n",
    "\n",
    "            Use the provided tools to extract information systematically.\n",
    "            \"\"\"\n",
    "        else: \n",
    "            extraction_prompt = f\"\"\"\n",
    "            You are an expert at extracting structured information from unstructured text.\n",
    "\n",
    "            Extract information from the following text and structure it according to the provided schema.\n",
    "\n",
    "            IMPORTANT INSTRUCTIONS:\n",
    "            1. Extract ALL relevant information you can find\n",
    "            2. Use the exact field names from the schema\n",
    "            3. Format dates as YYYY-MM-DD or YYYY-MM or YYYY\n",
    "            4. If information is not available, use null or empty string\n",
    "            5. Be thorough and accurate\n",
    "\n",
    "            Schema to follow:\n",
    "            {json.dumps(schema, indent=2)}\n",
    "\n",
    "            Text to extract from:\n",
    "            {chunk}\n",
    "\n",
    "            Use the provided tools to extract information systematically.\n",
    "            \"\"\"\n",
    "\n",
    "        messages = [HumanMessage(extraction_prompt)]\n",
    "\n",
    "        ai_response = llm_with_doc_tools.invoke(messages)\n",
    "\n",
    "        if hasattr(ai_response, 'tool_calls') and ai_response.tool_calls:\n",
    "            print(f\"Found {len(ai_response.tool_calls)} tool calls\")\n",
    "            chunk_results = {}\n",
    "\n",
    "            for tool_call in ai_response.tool_calls:\n",
    "                tool_name = tool_call['name']\n",
    "                tool_args = tool_call['args']\n",
    "                print(f\"Extracted {tool_name}: {tool_args}\")\n",
    "\n",
    "                if tool_name not in chunk_results:\n",
    "                    chunk_results[tool_name] = []\n",
    "                chunk_results[tool_name].append(tool_args)\n",
    "\n",
    "            all_results.append(chunk_results)\n",
    "        else:\n",
    "            print(\"No structured data extracted from this chunk\")\n",
    "\n",
    "    if doc_type == \"github_action\":\n",
    "        final_result = merge_github_action_results(all_results)\n",
    "    else:\n",
    "        final_result = merge_chunk_results(all_results)  \n",
    "\n",
    "    return final_result\n",
    "\n",
    "print(\"Universal document processor defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "993ecf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced universal pipeline defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_document_universal(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Universal pipeline to process any supported document type.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the document file\n",
    "\n",
    "    Returns:\n",
    "        Structured JSON data following the appropriate schema\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Processing document: {file_path}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # \n",
    "        file_type = identify_file_type_enhanced(file_path)\n",
    "        doc_type, schema, processing_tools = determine_document_type_and_schema(file_path, file_type)\n",
    "\n",
    "        print(f\"Document analyzed successfully!\")\n",
    "        print(f\"   - File type: {file_type}\")\n",
    "        print(f\"   - Document type: {doc_type}\")\n",
    "        print(f\"   - Schema sections: {len(schema.get('properties', {}))}\")\n",
    "\n",
    "\n",
    "        text = parse_file_enhanced(file_path, file_type)\n",
    "\n",
    "        print(f\"✅ File parsed successfully!\")\n",
    "        print(f\"   - Text length: {len(text)} characters\")\n",
    "        print(f\"   - Token count: {count_tokens(text)}\")\n",
    "\n",
    "        structured_data = convert_document_to_json(\n",
    "            text, doc_type, schema, processing_tools\n",
    "        )\n",
    "\n",
    "        print(\"\\nConversion completed!\")\n",
    "\n",
    "        # Step 4: Validate against schema (basic validation)\n",
    "        print(\"\\nStep 4: Basic validation...\")\n",
    "\n",
    "        if doc_type == \"github_action\":\n",
    "            required_sections = [\"name\", \"description\", \"runs\"]\n",
    "        else:  \n",
    "            required_sections = [\"basics\", \"work\", \"education\", \"skills\", \"projects\"]\n",
    "\n",
    "        validation_results = {}\n",
    "        for section in required_sections:\n",
    "            if section in structured_data and structured_data[section]:\n",
    "                if isinstance(structured_data[section], dict):\n",
    "                    has_content = any(v for v in structured_data[section].values() if v)\n",
    "                    validation_results[section] = \"✅ Present\" if has_content else \"⚠️ Empty\"\n",
    "                elif isinstance(structured_data[section], list):\n",
    "                    validation_results[section] = \"✅ Present\" if structured_data[section] else \"⚠️ Empty\"\n",
    "                else:\n",
    "                    validation_results[section] = \"✅ Present\" if structured_data[section] else \"⚠️ Empty\"\n",
    "            else:\n",
    "                validation_results[section] = \"⚠️ Missing\"\n",
    "\n",
    "        print(\"Validation Results:\")\n",
    "        for section, status in validation_results.items():\n",
    "            print(f\"   - {section}: {status}\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"document_type\": doc_type,\n",
    "            \"data\": structured_data,\n",
    "            \"validation\": validation_results,\n",
    "            \"metadata\": {\n",
    "                \"file_type\": file_type,\n",
    "                \"original_length\": len(text),\n",
    "                \"token_count\": count_tokens(text),\n",
    "                \"processing_timestamp\": datetime.now().isoformat(),\n",
    "                \"schema_complexity\": analyze_schema_complexity(schema)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing document: {str(e)}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"data\": None\n",
    "        }\n",
    "\n",
    "print(\"Enhanced universal pipeline defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4413021d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test GitHub Action description created!\n",
      "\n",
      "Testing with GitHub Action description...\n",
      "Processing document: test_mkdocs_action.md\n",
      "==================================================\n",
      "Document analyzed successfully!\n",
      "   - File type: action_md\n",
      "   - Document type: github_action\n",
      "   - Schema sections: 7\n",
      "✅ File parsed successfully!\n",
      "   - Text length: 1201 characters\n",
      "   - Token count: 309\n",
      "Processing github_action with complex complexity...\n",
      "Input text tokens: 309\n",
      "Processing as single chunk\n",
      "\n",
      "Processing chunk 1/1...\n",
      "Found 11 tool calls\n",
      "Extracted GitHubActionInfo: {'name': 'MkDocs Publisher Action', 'purpose': 'A simple action to build an MkDocs site and push it to the gh-pages branch. Should be easy to use.', 'description': 'A simple action to build an MkDocs site and push it to the gh-pages branch. Should be easy to use.'}\n",
      "Extracted ActionInputOutput: {'name': 'python-version', 'required': False, 'description': 'The version of Python to set up for building.', 'default_value': '3.11'}\n",
      "Extracted ActionInputOutput: {'name': 'requirements-file', 'required': True, 'description': 'Path to the Python requirements file.', 'default_value': None}\n",
      "Extracted ActionInputOutput: {'name': 'gh-token', 'required': True, 'description': 'GitHub token for deployment.', 'default_value': None}\n",
      "Extracted ActionInputOutput: {'name': 'page-url', 'required': False, 'description': 'The URL of the deployed GitHub Pages site.', 'default_value': None}\n",
      "Extracted ActionStep: {'name': 'Checkout Code', 'action_type': 'uses', 'shell': None, 'condition': None, 'id': None, 'run_command': None, 'with_parameters': None, 'uses': 'actions/checkout@v4'}\n",
      "Extracted ActionStep: {'name': 'Setup Python', 'action_type': 'uses', 'condition': None, 'shell': None, 'id': None, 'run_command': None, 'with_parameters': 'python-version', 'uses': 'actions/setup-python@v5'}\n",
      "Extracted ActionStep: {'name': 'Install Dependencies', 'action_type': 'run', 'shell': 'bash', 'condition': None, 'id': None, 'uses': None, 'with_parameters': None, 'run_command': 'pip install -r ${{ inputs.requirements-file }}'}\n",
      "Extracted ActionStep: {'name': 'Build Site', 'action_type': 'run', 'shell': 'bash', 'condition': None, 'id': None, 'run_command': 'mkdocs build', 'with_parameters': None, 'uses': None}\n",
      "Extracted ActionStep: {'name': 'Deploy to Pages', 'action_type': 'uses', 'shell': None, 'condition': \"github.ref == 'refs/heads/main'\", 'id': None, 'run_command': None, 'with_parameters': 'github_token: ${{ inputs.gh-token }}\\npublish_dir: ./site', 'uses': 'peaceiris/actions-gh-pages@v3'}\n",
      "Extracted ActionBranding: {'color': 'blue', 'icon': 'book-open'}\n",
      "\n",
      "Conversion completed!\n",
      "\n",
      "Step 4: Basic validation...\n",
      "Validation Results:\n",
      "   - name: ✅ Present\n",
      "   - description: ✅ Present\n",
      "   - runs: ✅ Present\n",
      "\n",
      "SUCCESS! GitHub Action processed successfully!\n",
      "\n",
      "DOCUMENT TYPE: github_action\n",
      "==================================================\n",
      "{\n",
      "  \"name\": \"MkDocs Publisher Action\",\n",
      "  \"author\": \"\",\n",
      "  \"description\": \"A simple action to build an MkDocs site and push it to the gh-pages branch. Should be easy to use.\",\n",
      "  \"branding\": {\n",
      "    \"icon\": \"book-open\",\n",
      "    \"color\": \"blue\"\n",
      "  },\n",
      "  \"inputs\": {\n",
      "    \"python-version\": {\n",
      "      \"description\": \"The version of Python to set up for building.\",\n",
      "      \"required\": false,\n",
      "      \"default\": \"3.11\"\n",
      "    },\n",
      "    \"requirements-file\": {\n",
      "      \"description\": \"Path to the Python requirements file.\",\n",
      "      \"required\": true\n",
      "    },\n",
      "    \"gh-token\": {\n",
      "      \"description\": \"GitHub token for deployment.\",\n",
      "      \"required\": true\n",
      "    }\n",
      "  },\n",
      "  \"outputs\": {\n",
      "    \"page-url\": {\n",
      "      \"description\": \"The URL of the deployed GitHub Pages site.\",\n",
      "      \"value\": null\n",
      "    }\n",
      "  },\n",
      "  \"runs\": {\n",
      "    \"using\": \"composite\",\n",
      "    \"steps\": [\n",
      "      {\n",
      "        \"name\": \"Checkout Code\",\n",
      "        \"uses\": \"actions/checkout@v4\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Setup Python\",\n",
      "        \"uses\": \"actions/setup-python@v5\",\n",
      "        \"with\": \"python-version\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Install Dependencies\",\n",
      "        \"run\": \"pip install -r ${{ inputs.requirements-file }}\",\n",
      "        \"shell\": \"bash\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Build Site\",\n",
      "        \"run\": \"mkdocs build\",\n",
      "        \"shell\": \"bash\"\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"Deploy to Pages\",\n",
      "        \"uses\": \"peaceiris/actions-gh-pages@v3\",\n",
      "        \"with\": \"github_token: ${{ inputs.gh-token }}\\npublish_dir: ./site\",\n",
      "        \"if\": \"github.ref == 'refs/heads/main'\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      " GitHub Action results saved to: extracted_github_action.json\n"
     ]
    }
   ],
   "source": [
    "test_action_md = \"\"\"\n",
    "# MkDocs Publisher Action\n",
    "\n",
    "A simple action to build an MkDocs site and push it to the gh-pages branch. Should be easy to use.\n",
    "\n",
    "**Author**: DevRel Team\n",
    "\n",
    "## Purpose\n",
    "A simple action to build an MkDocs site and push it to the gh-pages branch. Should be easy to use.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "### `python-version`\n",
    "The version of Python to set up for building. Optional, defaults to 3.11.\n",
    "\n",
    "### `requirements-file`\n",
    "Path to the Python requirements file. **Required**.\n",
    "\n",
    "### `gh-token`\n",
    "GitHub token for deployment. **Required**. Usually `${{ secrets.GITHUB_TOKEN }}`.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "### `page-url`\n",
    "The URL of the deployed GitHub Pages site.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "This will be a composite action. The execution steps:\n",
    "\n",
    "1. **Checkout Code**: Use `actions/checkout@v4`\n",
    "2. **Setup Python**: Use `actions/setup-python@v5` with the specified python-version\n",
    "3. **Install Dependencies**: Run `pip install -r ${{ inputs.requirements-file }}` using bash shell\n",
    "4. **Build Site**: Run `mkdocs build` using bash\n",
    "5. **Deploy to Pages**: Use `peaceiris/actions-gh-pages@v3` with:\n",
    "   - github_token: `${{ inputs.gh-token }}`\n",
    "   - publish_dir: `./site`\n",
    "   - condition: `github.ref == 'refs/heads/main'`\n",
    "\n",
    "## Branding\n",
    "- Color: blue\n",
    "- Icon: book-open\n",
    "\"\"\"\n",
    "\n",
    "test_file_path = \"test_mkdocs_action.md\"\n",
    "with open(test_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(test_action_md)\n",
    "\n",
    "print(\"Test GitHub Action description created!\")\n",
    "\n",
    "print(\"\\nTesting with GitHub Action description...\")\n",
    "action_result = process_document_universal(test_file_path)\n",
    "\n",
    "if action_result[\"status\"] == \"success\":\n",
    "    print(\"\\nSUCCESS! GitHub Action processed successfully!\")\n",
    "    print(f\"\\nDOCUMENT TYPE: {action_result['document_type']}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    structured_json = json.dumps(action_result[\"data\"], indent=2, ensure_ascii=False)\n",
    "    print(structured_json)\n",
    "\n",
    "    action_output_filename = \"extracted_github_action.json\"\n",
    "    with open(action_output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(action_result[\"data\"], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n GitHub Action results saved to: {action_output_filename}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n FAILED to process GitHub Action:\")\n",
    "    print(f\"   Error: {action_result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "99484d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch processing function defined!\n"
     ]
    }
   ],
   "source": [
    "def process_multiple_documents(file_paths: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process multiple documents and return consolidated results.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results for each file\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"processed_files\": [],\n",
    "        \"successful\": [],\n",
    "        \"failed\": [],\n",
    "        \"summary\": {\n",
    "            \"total_files\": len(file_paths),\n",
    "            \"successful_count\": 0,\n",
    "            \"failed_count\": 0,\n",
    "            \"document_types\": {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\" Processing {len(file_paths)} documents...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        print(f\"\\n📄 Processing file {i+1}/{len(file_paths)}: {file_path}\")\n",
    "        \n",
    "        result = process_document_universal(file_path)\n",
    "        \n",
    "        file_result = {\n",
    "            \"file_path\": file_path,\n",
    "            \"result\": result\n",
    "        }\n",
    "        \n",
    "        results[\"processed_files\"].append(file_result)\n",
    "        \n",
    "        if result[\"status\"] == \"success\":\n",
    "            results[\"successful\"].append(file_path)\n",
    "            results[\"summary\"][\"successful_count\"] += 1\n",
    "            \n",
    "            # Track document types\n",
    "            doc_type = result.get(\"document_type\", \"unknown\")\n",
    "            if doc_type not in results[\"summary\"][\"document_types\"]:\n",
    "                results[\"summary\"][\"document_types\"][doc_type] = 0\n",
    "            results[\"summary\"][\"document_types\"][doc_type] += 1\n",
    "            \n",
    "            print(f\" Success: {doc_type}\")\n",
    "        else:\n",
    "            results[\"failed\"].append(file_path)\n",
    "            results[\"summary\"][\"failed_count\"] += 1\n",
    "            print(f\" Failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BATCH PROCESSING SUMMARY:\")\n",
    "    print(f\"   - Total files: {results['summary']['total_files']}\")\n",
    "    print(f\"   - Successful: {results['summary']['successful_count']}\")\n",
    "    print(f\"   - Failed: {results['summary']['failed_count']}\")\n",
    "    print(f\"   - Document types processed: {results['summary']['document_types']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_files = [\n",
    "    \"C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\JankiGabaniResume_.pdf\",\n",
    "    \"test_mkdocs_action.md\"  \n",
    "]\n",
    "\n",
    "print(\"Batch processing function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7239684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance analysis function defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_system_performance(results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze system performance based on processing results.\n",
    "    \n",
    "    Args:\n",
    "        results: Results from document processing\n",
    "        \n",
    "    Returns:\n",
    "        Performance analysis report\n",
    "    \"\"\"\n",
    "    \n",
    "    performance_data = {\n",
    "        \"complexity_distribution\": {},\n",
    "        \"token_distribution\": {},\n",
    "        \"processing_efficiency\": {},\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    total_tokens = 0\n",
    "    total_files = 0\n",
    "    complexity_counts = {\"simple\": 0, \"medium\": 0, \"complex\": 0}\n",
    "    \n",
    "    for file_result in results.get(\"processed_files\", []):\n",
    "        if file_result[\"result\"][\"status\"] == \"success\":\n",
    "            metadata = file_result[\"result\"][\"metadata\"]\n",
    "            \n",
    "            tokens = metadata[\"token_count\"]\n",
    "            total_tokens += tokens\n",
    "            total_files += 1\n",
    "            \n",
    "            if tokens < 1000:\n",
    "                category = \"small\"\n",
    "            elif tokens < 5000:\n",
    "                category = \"medium\"  \n",
    "            elif tokens < 15000:\n",
    "                category = \"large\"\n",
    "            else:\n",
    "                category = \"xl\"\n",
    "                \n",
    "            if category not in performance_data[\"token_distribution\"]:\n",
    "                performance_data[\"token_distribution\"][category] = 0\n",
    "            performance_data[\"token_distribution\"][category] += 1\n",
    "            \n",
    "            complexity = metadata[\"schema_complexity\"][\"complexity\"]\n",
    "            complexity_counts[complexity] += 1\n",
    "    \n",
    "    if total_files > 0:\n",
    "        performance_data[\"processing_efficiency\"] = {\n",
    "            \"average_tokens_per_file\": total_tokens / total_files,\n",
    "            \"total_files_processed\": total_files,\n",
    "            \"complexity_distribution\": complexity_counts\n",
    "        }\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if complexity_counts[\"complex\"] > complexity_counts[\"simple\"]:\n",
    "        recommendations.append(\n",
    "            \"Consider implementing adaptive chunking for complex schemas\"\n",
    "        )\n",
    "    \n",
    "    if performance_data[\"token_distribution\"].get(\"xl\", 0) > 0:\n",
    "        recommendations.append(\n",
    "            \"Large documents detected. Consider parallel processing for better performance\"\n",
    "        )\n",
    "    \n",
    "    if total_files > 10:\n",
    "        recommendations.append(\n",
    "            \"For batch processing, consider implementing async processing\"\n",
    "        )\n",
    "    \n",
    "    performance_data[\"recommendations\"] = recommendations\n",
    "    \n",
    "    return performance_data\n",
    "\n",
    "print(\"Performance analysis function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0a0f0044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUPPORTED DOCUMENT TYPES:\n",
      "   - RESUME: ['PDF', 'DOCX', 'TXT'] | 5 sections | complex complexity\n",
      "   - GITHUB_ACTION: ['MD', 'YAML'] | 7 sections | complex complexity\n",
      "\n",
      "SYSTEM SPECIFICATIONS:\n",
      "   - Max context window: 25,000 tokens\n",
      "   - Chunk size: 15,000 tokens (with 500 overlap)\n",
      "   - Supported nesting levels: 3-7 levels\n",
      "   - Max nested objects: 50-150 objects\n",
      "   - Document size limit: Up to 10MB\n",
      "\n",
      "USAGE EXAMPLES:\n",
      "   - Single document: process_document_universal('path/to/file')\n",
      "   - Batch processing: process_multiple_documents(['file1', 'file2'])\n",
      "   - Resume only: process_resume_file('resume.pdf')\n",
      "\n",
      "SYSTEM READY FOR PRODUCTION USE!\n",
      "   All components tested and validated.\n",
      "   Ready to handle complex B2B document processing workflows.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSUPPORTED DOCUMENT TYPES:\")\n",
    "document_types = {\n",
    "    \"resume\": {\n",
    "        \"formats\": [\"PDF\", \"DOCX\", \"TXT\"],\n",
    "        \"schema_sections\": len(RESUME_SCHEMA[\"properties\"]),\n",
    "        \"complexity\": analyze_schema_complexity(RESUME_SCHEMA)[\"complexity\"]\n",
    "    },\n",
    "    \"github_action\": {\n",
    "        \"formats\": [\"MD\", \"YAML\"],\n",
    "        \"schema_sections\": len(GITHUB_ACTION_SCHEMA[\"properties\"]),\n",
    "        \"complexity\": analyze_schema_complexity(GITHUB_ACTION_SCHEMA)[\"complexity\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for doc_type, info in document_types.items():\n",
    "    print(f\"   - {doc_type.upper()}: {info['formats']} | {info['schema_sections']} sections | {info['complexity']} complexity\")\n",
    "\n",
    "print(f\"\\nSYSTEM SPECIFICATIONS:\")\n",
    "print(f\"   - Max context window: 25,000 tokens\")\n",
    "print(f\"   - Chunk size: 15,000 tokens (with 500 overlap)\")\n",
    "print(f\"   - Supported nesting levels: 3-7 levels\")\n",
    "print(f\"   - Max nested objects: 50-150 objects\")\n",
    "print(f\"   - Document size limit: Up to 10MB\")\n",
    "\n",
    "print(f\"\\nUSAGE EXAMPLES:\")\n",
    "print(f\"   - Single document: process_document_universal('path/to/file')\")\n",
    "print(f\"   - Batch processing: process_multiple_documents(['file1', 'file2'])\")\n",
    "print(f\"   - Resume only: process_resume_file('resume.pdf')\")\n",
    "\n",
    "print(\"\\nSYSTEM READY FOR PRODUCTION USE!\")\n",
    "print(\"   All components tested and validated.\")\n",
    "print(\"   Ready to handle complex B2B document processing workflows.\")\n",
    "\n",
    "# Cleanup test file\n",
    "if os.path.exists(\"test_mkdocs_action.md\"):\n",
    "    os.remove(\"test_mkdocs_action.md\")\n",
    "    print(\"\\nTest files cleaned up.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
